{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NagillaUdayasree/Udayasree_INFO5731_Spring2024/blob/main/Udayasree_Nagilla_Exercise_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DymRJbxDBCnf"
      },
      "source": [
        "# **INFO5731 In-class Exercise 2**\n",
        "\n",
        "The purpose of this exercise is to understand users' information needs, and then collect data from different sources for analysis by implementing web scraping using Python.\n",
        "\n",
        "**Expectations**:\n",
        "*   Students are expected to complete the exercise during lecture period to meet the active participation criteria of the course.\n",
        "*   Use the provided .*ipynb* document to write your code & respond to the questions. Avoid generating a new file.\n",
        "*   Write complete answers and run all the cells before submission.\n",
        "*   Make sure the submission is \"clean\"; *i.e.*, no unnecessary code cells.\n",
        "*   Once finished, allow shared rights from top right corner (*see Canvas for details*).\n",
        "\n",
        "**Total points**: 40\n",
        "\n",
        "**Deadline**: This in-class exercise is due at the end of the day tomorrow, at 11:59 PM.\n",
        "\n",
        "**Late submissions will have a penalty of 10% of the marks for each day of late submission. , and no requests will be answered. Manage your time accordingly.**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 1 (10 Points)\n",
        "Describe an interesting research question (or practical question or something innovative) you have in mind, what kind of data should be collected to answer the question(s)? Specify the amount of data needed for analysis. Provide detailed steps for collecting and saving the data."
      ],
      "metadata": {
        "id": "FBKvD6O_TY6e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Research Question : \"How do lifestyle factors such as diet, exercise, and stress levels impact the risk of developing heart disease?\"\n",
        "\n",
        "we would need to collect data on various lifestyle factors, medical history, and heart disease outcomes. Here are the steps for collecting and saving the data:\n",
        "\n",
        "Identify Relevant Variables: Determine which variables are relevant to the research question. This may include factors such as age, gender, BMI, diet (e.g., consumption of fruits, vegetables, saturated fats), exercise habits (e.g., frequency, duration), smoking status, alcohol consumption, stress levels, family history of heart disease, blood pressure, cholesterol levels, and presence of other medical conditions.\n",
        "\n",
        "Find Suitable Data Sources: Look for datasets that contain information on the identified variables. These datasets can come from various sources such as health surveys, medical records, clinical trials, or population studies.\n",
        "\n",
        "Acquire the Data: Download or access the datasets containing the relevant variables. Ensure that the datasets are in a format that can be easily imported into Python, such as CSV or Excel.\n",
        "\n",
        "Preprocess the Data: Preprocess the datasets by cleaning missing values, standardizing variable names, and formatting data types as needed. Merge or concatenate multiple datasets if necessary.\n",
        "\n",
        "Select Sample Size: Determine the sample size needed for analysis. Since the research question involves examining the impact of lifestyle factors on heart disease risk, a large sample size may be required to capture a diverse range of individuals with varying lifestyles.\n",
        "\n",
        "Select Data Analysis Methods: Decide on the appropriate data analysis methods for addressing the research question. This may include descriptive statistics, correlation analysis, regression analysis, or machine learning techniques for predictive modeling.\n",
        "\n",
        "Save the Data: Once the data preprocessing is complete, save the cleaned and merged dataset to a CSV file for further analysis. Ensure that the file is well-documented and includes information about variable definitions and data sources."
      ],
      "metadata": {
        "id": "cikVKDXdTbzE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 2 (10 Points)\n",
        "Write Python code to collect a dataset of 1000 samples related to the question discussed in Question 1."
      ],
      "metadata": {
        "id": "E9RqrlwdTfvl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import requests\n",
        "from io import StringIO\n",
        "\n",
        "# URL of the raw dataset file on GitHub\n",
        "url = \"https://raw.githubusercontent.com/hosiajosindra/heart-attack-classification/main/heart.csv\"\n",
        "\n",
        "# Fetch the data from the URL\n",
        "response = requests.get(url)\n",
        "\n",
        "# Check if the request was successful\n",
        "if response.status_code == 200:\n",
        "    # Read the content of the response into a DataFrame\n",
        "    heart_disease_data = pd.read_csv(StringIO(response.text))\n",
        "\n",
        "    # Display the first few rows of the dataset\n",
        "    print(heart_disease_data.head())\n",
        "\n",
        "    # Select a representative sample of 1000 rows\n",
        "    sample_data = heart_disease_data.sample(n=1000,replace=True, random_state=42)\n",
        "\n",
        "    # Save the sample dataset to a CSV file\n",
        "    sample_data.to_csv('heart_disease_sample.csv', index=False)\n",
        "\n",
        "    print(\"Sample dataset saved successfully.\")\n",
        "else:\n",
        "    print(\"Error: Unable to fetch data from the URL\")\n"
      ],
      "metadata": {
        "id": "4XvRknixTh1g",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "939fb344-1c9a-41a6-d138-3989afe05fb1"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   age  sex  cp  trtbps  chol  fbs  restecg  thalachh  exng  oldpeak  slp  \\\n",
            "0   63    1   3     145   233    1        0       150     0      2.3    0   \n",
            "1   37    1   2     130   250    0        1       187     0      3.5    0   \n",
            "2   41    0   1     130   204    0        0       172     0      1.4    2   \n",
            "3   56    1   1     120   236    0        1       178     0      0.8    2   \n",
            "4   57    0   0     120   354    0        1       163     1      0.6    2   \n",
            "\n",
            "   caa  thall  output  \n",
            "0    0      1       1  \n",
            "1    0      2       1  \n",
            "2    0      2       1  \n",
            "3    0      2       1  \n",
            "4    0      2       1  \n",
            "Sample dataset saved successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "03jb4GZsBkBS"
      },
      "source": [
        "## Question 3 (10 Points)\n",
        "Write Python code to collect 1000 articles from Google Scholar (https://scholar.google.com/), Microsoft Academic (https://academic.microsoft.com/home), or CiteSeerX (https://citeseerx.ist.psu.edu/index), or Semantic Scholar (https://www.semanticscholar.org/), or ACM Digital Libraries (https://dl.acm.org/) with the keyword \"XYZ\". The articles should be published in the last 10 years (2014-2024).\n",
        "\n",
        "The following information from the article needs to be collected:\n",
        "\n",
        "(1) Title of the article\n",
        "\n",
        "(2) Venue/journal/conference being published\n",
        "\n",
        "(3) Year\n",
        "\n",
        "(4) Authors\n",
        "\n",
        "(5) Abstract"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "YaGLbSHHB8Ej",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0321d5e5-dc9a-4666-d1bc-1a40d24c9366"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Article 1:\n",
            "Title: An overview of XYZ new particles\n",
            "Venue: Chinese Science Bulletin, 2014\n",
            "Year: 2014\n",
            "Authors: \n",
            "Abstract: … (XYZ\\) have been announced by experiments after analyzing various processes. Until now, \n",
            "the family of \\(XYZ… In general, the observed \\(XYZ\\) states can be categorized into five groups, …\n",
            "\n",
            "\n",
            "Article 2:\n",
            "Title: The XYZ states: experimental and theoretical status and perspectives\n",
            "Venue: Physics Reports, 2020\n",
            "Year: 2020\n",
            "Authors: N Brambilla, S Eidelman\n",
            "Abstract: The quark model was formulated in 1964 to classify mesons as bound states made of a \n",
            "quark–antiquark pair, and baryons as bound states made of three quarks. For a long time all …\n",
            "\n",
            "\n",
            "Article 3:\n",
            "Title: [HTML][HTML] XYZ states: An experimental point-of-view\n",
            "Venue: Reviews in Physics, 2022\n",
            "Year: 2022\n",
            "Authors: \n",
            "Abstract: Since 2003, a new family of states without a clear theoretical interpretation has been measured \n",
            "in the heavy quarkonium spectrum, the so-called X Y Z states. While the nature of these …\n",
            "\n",
            "\n",
            "Article 4:\n",
            "Title: Implementation technology for development of a brand communication in company pt. xyz\n",
            "Venue: Aptisi Transactions on …, 2022\n",
            "Year: 2022\n",
            "Authors: CS Kesumawati\n",
            "Abstract: The study was conducted at Catering Service Company. The high competition of catering \n",
            "business requires the right brand communication strategy to communicate the uniqueness of …\n",
            "\n",
            "\n",
            "Article 5:\n",
            "Title: Born-Oppenheimer approximation for the  mesons\n",
            "Venue: Physical Review D, 2014\n",
            "Year: 2014\n",
            "Authors: DH Smith\n",
            "Abstract: … In this paper, we apply the BO approximation for quarkonium hybrids and tetraquarks to the \n",
            "XYZ mesons. In Sec. II, we list the XYZ mesons that have been observed so far. In Sec. III, we …\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "import time\n",
        "\n",
        "def scrape_articles(keyword, num_articles_to_collect):\n",
        "    base_url = \"https://scholar.google.com\"\n",
        "    search_query = f\"{base_url}/scholar?q={keyword}&as_ylo=2014&as_yhi=2024&hl=en&as_sdt=0,5\"\n",
        "\n",
        "    collected_articles = []\n",
        "    collected_articles_count = 0\n",
        "\n",
        "    while collected_articles_count < num_articles_to_collect:\n",
        "        # Send a GET request to the search URL\n",
        "        response = requests.get(search_query)\n",
        "\n",
        "        # Check if the request was successful\n",
        "        if response.status_code == 200:\n",
        "            # Parse the HTML content of the page\n",
        "            soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "            # Find all the article elements on the page\n",
        "            article_elements = soup.find_all('div', class_='gs_ri')\n",
        "\n",
        "            for element in article_elements:\n",
        "                # Extract article details if available\n",
        "                title = element.find('h3', class_='gs_rt').text.strip()\n",
        "\n",
        "                venue_element = element.find('div', class_='gs_a')\n",
        "                if venue_element:\n",
        "                    venue_match = re.search(r'-\\s*(.*?)\\s*-', venue_element.text)\n",
        "                    if venue_match:\n",
        "                        venue = venue_match.group(1).strip()\n",
        "                    else:\n",
        "                        venue = \"Venue not available\"\n",
        "                else:\n",
        "                    venue = \"Venue not available\"\n",
        "\n",
        "                year_match = re.search(r'\\d{4}', venue_element.text)\n",
        "                year = year_match.group() if year_match else \"Year not available\"\n",
        "\n",
        "                authors_element = element.find('div', class_='gs_a').find_all('a')\n",
        "                authors = ', '.join([author.text.strip() for author in authors_element])\n",
        "\n",
        "                abstract_element = element.find('div', class_='gs_rs')\n",
        "                abstract = abstract_element.text.strip() if abstract_element else \"Abstract not available\"\n",
        "\n",
        "                # Append article details to the list\n",
        "                collected_articles.append({\n",
        "                    'title': title,\n",
        "                    'venue': venue,\n",
        "                    'year': year,\n",
        "                    'authors': authors,\n",
        "                    'abstract': abstract\n",
        "                })\n",
        "\n",
        "                collected_articles_count += 1\n",
        "\n",
        "                if collected_articles_count == num_articles_to_collect:\n",
        "                    break\n",
        "\n",
        "            # Check if there are more pages of results\n",
        "            next_page_link = soup.find('a', class_='gs_ico gs_ico_nav_next')\n",
        "            if next_page_link:\n",
        "                search_query = base_url + next_page_link['href']\n",
        "            else:\n",
        "                break\n",
        "        else:\n",
        "            print(\"Error: Unable to retrieve articles\")\n",
        "            break\n",
        "\n",
        "        # Add a delay to avoid overwhelming the server\n",
        "        time.sleep(1)\n",
        "\n",
        "    return collected_articles\n",
        "\n",
        "# Example usage\n",
        "search_keyword = \"XYZ\"\n",
        "num_articles_to_collect = 1000\n",
        "articles = scrape_articles(search_keyword, num_articles_to_collect)\n",
        "\n",
        "# Save the collected articles to a file\n",
        "with open('collected_articles.txt', 'w', encoding='utf-8') as file:\n",
        "    for article in articles:\n",
        "        file.write(f\"Title: {article['title']}\\n\")\n",
        "        file.write(f\"Venue: {article['venue']}\\n\")\n",
        "        file.write(f\"Year: {article['year']}\\n\")\n",
        "        file.write(f\"Authors: {article['authors']}\\n\")\n",
        "        file.write(f\"Abstract: {article['abstract']}\\n\\n\")\n",
        "\n",
        "# Output the first few articles\n",
        "for i in range(5):\n",
        "    print(f\"Article {i+1}:\")\n",
        "    print(f\"Title: {articles[i]['title']}\")\n",
        "    print(f\"Venue: {articles[i]['venue']}\")\n",
        "    print(f\"Year: {articles[i]['year']}\")\n",
        "    print(f\"Authors: {articles[i]['authors']}\")\n",
        "    print(f\"Abstract: {articles[i]['abstract']}\")\n",
        "    print(\"\\n\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jJDe71iLB616"
      },
      "source": [
        "## Question 4A (10 Points)\n",
        "Develop Python code to collect data from social media platforms like Reddit, Instagram, Twitter (formerly known as X), Facebook, or any other. Use hashtags, keywords, usernames, or user IDs to gather the data.\n",
        "\n",
        "\n",
        "\n",
        "Ensure that the collected data has more than four columns.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "MtKskTzbCLaU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ed1a091d-6f32-41d7-afde-547b8ea3046c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "b\"<!doctype html>\\n     <html>\\n  <head>\\n    <title>Blocked</title>\\n    <style>\\n      body {\\n          font: small verdana, arial, helvetica, sans-serif;\\n          width: 600px;\\n          margin: 0 auto;\\n      }\\n\\n      h1 {\\n          height: 40px;\\n          background: transparent url(//www.redditstatic.com/reddit.com.header.png) no-repeat scroll top right;\\n      }\\n    </style>\\n  </head>\\n  <body>\\n    <h1>whoa there, pardner!</h1>\\n\\n<p>Your request has been blocked due to a network policy.</p>\\n\\n<p>Try logging in or creating an account <a href=https://www.reddit.com/login/>here</a> to get back to browsing.</p>\\n\\n<p>If you're running a script or application, please register or sign in with your developer credentials <a href=https://www.reddit.com/wiki/api/>here</a>. Additionally make sure your User-Agent is not empty and is something unique and descriptive and try again. if you're supplying an alternate User-Agent string,\\ntry changing back to default as that can sometimes result in a block.</p>\\n\\n<p>You can read Reddit's Terms of Service <a href=https://www.reddit.com/wiki/api/>here</a>.</p>\\n\\n<p>if you think that we've incorrectly blocked you or you would like to discuss\\neasier ways to get the data you want, please file a ticket <a href=https://support.reddithelp.com/hc/en-us/requests/new?ticket_form_id=21879292693140>here</a>.</p>\\n\\n<p>when contacting us, please include your ip address which is: <strong>34.31.236.210</strong> and reddit account</p>\\n  </body>\\n</html>\\n    \"\n",
            "No. of posts extracted were 0\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# write your answer here\n",
        "import json\n",
        "from datetime import datetime\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "headers = {\n",
        "    'authority': 'www.reddit.com',\n",
        "    'accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7',\n",
        "    'accept-language': 'en-US,en;q=0.9',\n",
        "    'cache-control': 'max-age=0',\n",
        "    # 'cookie': 'rdt=d9156fa0768a757557ad793f28748e0e; edgebucket=arLuX6jp7DGYl2A688; loid=000000000ua7lqq3vy.2.1708044555156.Z0FBQUFBQmx6ckVMX0I2OWdqU1U1SWFGMVlXaVJFMlBLNjEzOTlWWkFQWkZJTGJlcVMxNC1IUmFJU0N5a0RjbWw3TGZNNmoxV0RrODJkOFpzZ3AzUjBkSzNNZVU0SUxMQVA0M0dQM1RVNFl6cGlvdkZvSjg5ZFh4aDdRRkdxU2dKdkxTSkEtc0c5YjU; token_v2=eyJhbGciOiJSUzI1NiIsImtpZCI6IlNIQTI1NjpzS3dsMnlsV0VtMjVmcXhwTU40cWY4MXE2OWFFdWFyMnpLMUdhVGxjdWNZIiwidHlwIjoiSldUIn0.eyJzdWIiOiJsb2lkIiwiZXhwIjoxNzA4MTMwOTU1LjE1NzE5NiwiaWF0IjoxNzA4MDQ0NTU1LjE1NzE5NiwianRpIjoiSjBRa09SYWw4Q0phSmt2TmJYV0c3a3dMZEpjcVZBIiwiY2lkIjoiMFItV0FNaHVvby1NeVEiLCJsaWQiOiJ0Ml91YTdscXEzdnkiLCJsY2EiOjE3MDgwNDQ1NTUxNTYsInNjcCI6ImVKeGtrZEdPdERBSWhkLWwxejdCX3lwX05odHNjWWFzTFFhb2szbjdEVm9jazcwN2NMNGlIUDhuS0lxRkxFMnVCS0drS1dFRld0T1VOaUx2NTh5OU9aRUZTeUZUUjg0M3l3b2thVXBQVW1ONXB5bFJ3V1prTGxmYXNVS0RCNllwVlM2WjIwS1BTNXZRM0kxRnowNk1xbHhXSHRUWW8zSnBiR01LMnhQanpjWnFReXF1eTZsTVlGa29uOFdMZnZ5Ry10WS1mN2JmaEhZd3JLZ0tEX1RPdUZ4d1lfSERGSGJfbnByMGJGMndxTDNYZzlRLTEtTjI3Yk5tb2RtNV9WelB2emFTY1RtRzVpZll2N3QtQ1IxNDVIbVpVUWN3WWcwX3lyQWo2X0N2T29ES0JRV01KWWhQSTVBcmwyX19KZGl1VGY4YXR5ZC0tR2JFVFdfNHJSbW81eExFb1VfajZ6Y0FBUF9fWERfZTR3IiwiZmxvIjoxfQ.NS6crHALde11wRKGX1LnAcZtvzwpjkeeAYfceMi7yvIwYG98ZNvHHFRfFuJ_iUnleI3ENUQ2VCCAWUMeN5-eASrFbJQ0JhxXfUft6kvajZsTqsX_CeOt1RxrBB3z9mG1nvq6qrAiBCWNQatClw2YWoEy0RLDxj5Tny5-ssY_5jkYph3NBwn8-KIGgLFoDJe5xxHGXSHcDwhLc4a84nxSpAnTf966CnRxXuhSJ4UH5z3_-nZ3PSutmG28yc9pU9WXvLuokC_07SJTP9Jah8Icl02SipXODRWaCo1wzzAmHUwLEh2UD-xPBwKGbc1Ib6FcOrIMSJQjfQ71py5nIvZcHQ; csv=2; csrf_token=bdc4813e75d19b2185e69da83990ca4b; session_tracker=fjfqjrbcemnechefgj.0.1708050577975.Z0FBQUFBQmx6c2lSbjlOel81MVNCX3FzRGlpekJ0VTl3REQ0WVdjekZPSjROeXVTcHZXeEc1RkNPcDNONzZyY3l5ZHdaOGI3Q0c2bFEtYS1BajFWLXh6M3RWYXp2bklCc21rWjF5XzlOWDhEa1I1RENXZ2MxbjhLNVYydThKeUFGangyRTdpQnNCRlg',\n",
        "    'sec-ch-ua': '\"Not A(Brand\";v=\"99\", \"Google Chrome\";v=\"121\", \"Chromium\";v=\"121\"',\n",
        "    'sec-ch-ua-mobile': '?0',\n",
        "    'sec-ch-ua-platform': '\"Windows\"',\n",
        "    'sec-fetch-dest': 'document',\n",
        "    'sec-fetch-mode': 'navigate',\n",
        "    'sec-fetch-site': 'none',\n",
        "    'sec-fetch-user': '?1',\n",
        "    'upgrade-insecure-requests': '1',\n",
        "    'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36',\n",
        "}\n",
        "\n",
        "\n",
        "query = \"XYZ\"\n",
        "url = f'https://www.reddit.com/search/?q={query}'\n",
        "page = requests.get(url, headers=headers)\n",
        "print(page.content)\n",
        "soup = BeautifulSoup(page.content, 'html.parser')\n",
        "\n",
        "search_params = {\n",
        "    'source': 'search',\n",
        "    'action': 'view',\n",
        "    'noun': 'post',\n",
        "    'data-testid': 'search-post'\n",
        "}\n",
        "faceplate_list = soup.findAll(name=\"faceplate-tracker\", attrs=search_params)\n",
        "\n",
        "print(f'No. of posts extracted were {len(faceplate_list)}')\n",
        "print()\n",
        "\n",
        "for faceplate in faceplate_list:\n",
        "    each_post_attr = faceplate.get('data-faceplate-tracking-context')\n",
        "\n",
        "    # load json\n",
        "    each_post_data = json.loads(each_post_attr)\n",
        "\n",
        "    post_data = each_post_data.get('post', {})\n",
        "    title = post_data.get('title', '')\n",
        "    date_timestamp = post_data.get('created_timestamp', '')\n",
        "    date = datetime.utcfromtimestamp(date_timestamp/1000.0).strftime('%Y-%m-%d %H:%M:%S')\n",
        "    score = post_data.get('score', 0)\n",
        "    subreddit_name = post_data.get('subreddit_name', '')\n",
        "    number_comments = post_data.get('number_comments', 0)\n",
        "\n",
        "    print(80*'#')\n",
        "    # Print the attributes\n",
        "    print(\"Title:\", title)\n",
        "    print(\"Date:\", date)\n",
        "    print(\"Score:\", score)\n",
        "    print(\"Subreddit Name:\", subreddit_name)\n",
        "    print(\"Number of Comments:\", number_comments)\n",
        "    print(80*'#')\n",
        "    print()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "55W9AMdXCSpV"
      },
      "source": [
        "## Question 4B (10 Points)\n",
        "If you encounter challenges with Question-4 web scraping using Python, employ any online tools such as ParseHub or Octoparse for data extraction. Introduce the selected tool, outline the steps for web scraping, and showcase the final output in formats like CSV or Excel.\n",
        "\n",
        "\n",
        "\n",
        "Upload a document (Word or PDF File) in any shared storage (preferably UNT OneDrive) and add the publicly accessible link in the below code cell.\n",
        "\n",
        "Please only choose one option for question 4. If you do both options, we will grade only the first one"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I57NXsauCec2"
      },
      "outputs": [],
      "source": [
        "# write your answer here\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Mandatory Question"
      ],
      "metadata": {
        "id": "sZOhks1dXWEe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Important: Reflective Feedback on Web Scraping and Data Collection**\n",
        "\n",
        "\n",
        "\n",
        "Please share your thoughts and feedback on the web scraping and data collection exercises you have completed in this assignment. Consider the following points in your response:\n",
        "\n",
        "\n",
        "\n",
        "Learning Experience: Describe your overall learning experience in working on web scraping tasks. What were the key concepts or techniques you found most beneficial in understanding the process of extracting data from various online sources?\n",
        "\n",
        "\n",
        "\n",
        "Challenges Encountered: Were there specific difficulties in collecting data from certain websites, and how did you overcome them? If you opted for the non-coding option, share your experience with the chosen tool.\n",
        "\n",
        "\n",
        "\n",
        "Relevance to Your Field of Study: How might the ability to gather and analyze data from online sources enhance your work or research?\n",
        "\n",
        "**(no grading of your submission if this question is left unanswered)**"
      ],
      "metadata": {
        "id": "eqmHVEwaWhbV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "The new concept presented a challenge initially, especially given the prerequisite knowledge required. However, delving deeply into the topic proved to be an enriching learning experience. I made efforts to comprehend the intricacies of HTML structure, focusing on how to precisely locate desired elements within it. Additionally, I explored the novel task of identifying the sources of datasets, which broadened my understanding. Yes, few websites din't allow to scrape directly and wanted to use API's. The code supported in few platforms and din't work on google collab.\n",
        "Collecting and analyzing data from online sources has been incredibly beneficial to me as an individual. It has provided me with access to a wealth of valuable information across different domains, enabling me to stay informed and make more informed decisions."
      ],
      "metadata": {
        "id": "akAVJn9YBTQT"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "il8kN9vE_2Cs"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}