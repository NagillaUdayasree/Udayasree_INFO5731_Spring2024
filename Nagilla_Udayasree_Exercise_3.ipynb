{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NagillaUdayasree/Udayasree_INFO5731_Spring2024/blob/main/Nagilla_Udayasree_Exercise_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VdRwkJBn70nX"
      },
      "source": [
        "# **INFO5731 In-class Exercise 3**\n",
        "\n",
        "The purpose of this exercise is to explore various aspects of text analysis, including feature extraction, feature selection, and text similarity ranking.\n",
        "\n",
        "**Expectations**:\n",
        "*   Students are expected to complete the exercise during lecture period to meet the active participation criteria of the course.\n",
        "*   Use the provided .*ipynb* document to write your code & respond to the questions. Avoid generating a new file.\n",
        "*   Write complete answers and run all the cells before submission.\n",
        "*   Make sure the submission is \"clean\"; *i.e.*, no unnecessary code cells.\n",
        "*   Once finished, allow shared rights from top right corner (*see Canvas for details*).\n",
        "\n",
        "**Total points**: 40\n",
        "\n",
        "**Deadline**: This in-class exercise is due at the end of the day tomorrow, at 11:59 PM.\n",
        "\n",
        "**Late submissions will have a penalty of 10% of the marks for each day of late submission. , and no requests will be answered. Manage your time accordingly.**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ARqm7u6B70ne"
      },
      "source": [
        "## Question 1 (10 Points)\n",
        "Describe an interesting text classification or text mining task and explain what kind of features might be useful for you to build the machine learning model. List your features and explain why these features might be helpful. You need to list at least five different types of features."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VAZj4PHB70nf"
      },
      "outputs": [],
      "source": [
        "# Your answer here (no code for this question, write down your answer as detail as possible for the above questions):\n",
        "\n",
        "'''\n",
        "Sorting news items into groups based on topics like politics, sports, technology, entertainment, and health could be a fun text categorization assignment. This task is identifying the topic or category to which news stories belong by examining their content. The following attributes could be helpful in developing a machine learning model for this task:\n",
        "\n",
        "Bag-of-Words (BoW) or TF-IDF Features:\n",
        "BoW features show how frequently each word appears in the article, whereas TF-IDF features use word frequency across all articles in the dataset to determine how important a word is.\n",
        "Why it's helpful: The language used in the articles is captured by the BoW and TF-IDF features, which also reveal whether or not certain terms are present in relation to other categories. Terms that fall within the category of politics include \"election,\" \"government,\" and \"policy,\" for instance.\n",
        "\n",
        "Word Embeddings:\n",
        "Word embeddings capture the semantic associations between words by representing them as dense, low-dimensional vectors in a continuous space.\n",
        "Why it's helpful:Word embeddings enable the model to capture subtle semantic information and similarities between words with comparable meanings by helping it grasp the context and meaning of the words in the articles. This may enhance the model's capacity to correctly categorize articles according to their content.\n",
        "\n",
        "Features of Named Entity Recognition (NER):\n",
        "Named entities in the articles, including individuals, groups, places, dates, and numerical expressions, are recognized and categorized by NER characteristics.\n",
        "Why it's helpful:Key entities discussed in the articles and their applicability to particular categories are detailed in NER features. Mentions of government agencies or political personalities, for instance, may point to a politics category.\n",
        "\n",
        "Features of Part-of-Speech (POS):\n",
        "POS characteristics classify words according to their grammatical categories (e.g., nouns, verbs, adjectives) in order to capture the syntactic structure of the articles.\n",
        "Why it's helpful: POS traits help the model's ability to recognize language cues connected to various categories and to capture syntactic patterns. An indication of a technology category could be, for example, a high frequency of adjectives characterizing technical developments.\n",
        "\n",
        "Features of Sentiment Analysis:\n",
        "\n",
        "Sentiment analysis elements are designed to capture the sentiment—whether positive, negative, or neutral—expressed in the articles.\n",
        "Why it's helpful:Although classifying news is the main goal, sentiment analysis features can offer further context and insights into the tone or position of the articles. An upbeat attitude toward technical progress, for instance, could be indicated by favorable sentiment in a technology article.\n",
        "\n",
        "The machine learning model can efficiently categorize news articles into distinct groups according to their content, entities referenced, syntactic structure, and sentiment expressed by integrating these aspects. With the help of this multifaceted approach, the model is better able to comprehend and categorize news stories, which makes tasks like trend analysis, subject modeling, and content suggestion easier.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dEUjBE6C70nf"
      },
      "source": [
        "## Question 2 (10 Points)\n",
        "Write python code to extract these features you discussed above. You can collect a few sample text data for the feature extraction."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 136,
      "metadata": {
        "id": "EoQX5s4O70nf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5f23cb83-a0e9-4456-d9bf-a8bc522943fa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bag-of-Words Features:\n",
            "[[1 1 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 1 0 1 0 0 0 0 0 1 0 0 1 0 0 0 0 0 1\n",
            "  0 0 0 0 0 1 0 0 0 0 0 1 0 0 2 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0\n",
            "  0 0 0 0 0 0 1 0 0 1 0 2 1 1 0 0 1 0 1]\n",
            " [0 0 0 1 1 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 1 1 1 1 0 1 0 0 0 0 0 0 0 0\n",
            "  1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0]\n",
            " [0 0 0 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 1 0 0 1 1 1 0 0\n",
            "  0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0]\n",
            " [0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0\n",
            "  0 0 1 1 1 0 0 1 1 0 1 1 0 0 0 0 0 0 0]]\n",
            "Feature Names: ['access' 'aims' 'all' 'annual' 'apple' 'at' 'blockbuster' 'box' 'broke'\n",
            " 'can' 'celebrated' 'citizens' 'city' 'disease' 'event' 'exercise'\n",
            " 'football' 'for' 'healthcare' 'heart' 'improve' 'inc' 'iphone' 'its'\n",
            " 'latest' 'launch' 'medical' 'model' 'movie' 'new' 'of' 'office' 'on'\n",
            " 'opening' 'parade' 'policy' 'product' 'records' 'reduce' 'regular' 'risk'\n",
            " 'services' 'streets' 'study' 'suggests' 'team' 'that' 'the' 'their'\n",
            " 'through' 'to' 'unveiled' 'victory' 'weekend' 'with']\n",
            "\n",
            "Word Embeddings:\n",
            "[[-2.48674229e-02 -1.64565504e-01 -1.45646811e-01  3.42455715e-01\n",
            "   1.15152337e-01 -4.44864444e-02  6.80895209e-01  1.12015963e-01\n",
            "   1.93035230e-01  2.53832519e-01  8.60573053e-02 -4.50651169e-01\n",
            "  -2.00431988e-01 -2.98031867e-01 -1.57486703e-02 -1.99388061e-02\n",
            "  -3.92466933e-01  2.05281124e-01 -1.72931731e-01 -3.22595909e-02\n",
            "  -1.24048181e-01 -1.66175574e-01 -2.39099160e-01 -1.26738846e-01\n",
            "   5.42606592e-01  2.64474601e-01  7.19179869e-01  4.96763706e-01\n",
            "   1.70558617e-01  3.74630034e-01 -2.01482475e-02 -2.67269224e-01\n",
            "   5.52288175e-01 -2.47339502e-01 -2.90366262e-01 -1.53929010e-01\n",
            "   3.44594926e-01  7.48507157e-02 -4.22834568e-02  1.89225957e-01\n",
            "  -1.92418978e-01  3.73096377e-01 -3.16057742e-01  3.12934723e-03\n",
            "   2.60212392e-01 -1.89081077e-02 -2.81916499e-01  2.32261002e-01\n",
            "   7.83352181e-02 -3.05154383e-01 -1.89204738e-01  1.60990909e-01\n",
            "   3.51674333e-02 -2.93626606e-01 -2.15740234e-01  1.93395004e-01\n",
            "  -3.41138363e-01  2.59287566e-01 -2.97789514e-01  1.35584921e-01\n",
            "   1.51995242e-01 -3.11956525e-01 -2.02279523e-01 -1.70080841e-01\n",
            "   3.33393127e-01 -2.14515373e-01  2.99870580e-01  7.93020725e-02\n",
            "  -1.98341116e-01  3.98986399e-01 -5.94626255e-02  1.03893243e-01\n",
            "  -1.71852913e-02  1.61259971e-03 -1.74521923e-01 -3.53118241e-01\n",
            "  -3.83572191e-01 -2.93133080e-01 -1.62056357e-01 -4.40016210e-01\n",
            "  -5.80714166e-01  1.26967043e-01 -6.27060980e-02 -1.38717398e-01\n",
            "   8.48041326e-02  1.42792895e-01 -2.78932601e-01  7.27950409e-02\n",
            "  -4.33005273e-01  6.05569556e-02 -2.70612448e-01  2.26835802e-01\n",
            "   6.36700213e-01  5.48366964e-01 -4.06649411e-01  5.01109213e-02]\n",
            " [ 3.60284038e-02 -5.19152619e-02 -1.46178111e-01  4.87047844e-02\n",
            "   8.05516765e-02  2.90809181e-02  3.89244288e-01  1.46839932e-01\n",
            "  -1.27629533e-01  2.00394005e-01  3.45829785e-01 -1.58781737e-01\n",
            "  -1.37916163e-01 -6.15394190e-02 -4.35776651e-01 -7.49413893e-02\n",
            "  -5.42698443e-01  3.46233666e-01  8.74280091e-03  5.82396500e-02\n",
            "  -2.97293365e-02  5.36045544e-02  1.34151861e-01 -3.91434968e-01\n",
            "   7.31857598e-01  2.28154995e-02  3.86003971e-01 -5.37704164e-03\n",
            "   2.26116583e-01  1.95084348e-01  1.47897797e-02 -4.37986851e-02\n",
            "   2.65690744e-01 -5.02547085e-01  6.51517957e-02 -1.74560800e-01\n",
            "   2.56011158e-01  8.67372602e-02 -2.75116980e-01  2.18069315e-01\n",
            "  -1.62479833e-01  5.58906734e-01  3.93907819e-03  3.38930577e-01\n",
            "  -1.92164540e-01  1.84664980e-01 -2.65079677e-01  5.97448766e-01\n",
            "   9.68784243e-02 -2.72452831e-01 -3.55119497e-01  1.60255097e-02\n",
            "   3.06292087e-01 -3.03950071e-01 -1.48415715e-01  1.31020173e-01\n",
            "  -3.64329070e-01  3.01914424e-01 -9.82528552e-02 -5.09036303e-01\n",
            "  -2.99082436e-02 -4.18177694e-01 -7.84057602e-02  6.96328655e-02\n",
            "   3.15300614e-01 -2.06640169e-01  1.38018981e-01 -1.04245730e-01\n",
            "  -5.21638356e-02  7.61457533e-02  3.57448161e-01  2.40513012e-01\n",
            "   7.36553669e-02 -2.91461170e-01 -1.75412521e-01 -2.69002974e-01\n",
            "  -3.66145521e-01 -2.65849382e-01 -1.95666984e-01 -4.82784659e-01\n",
            "  -2.05496084e-02  1.43838450e-01 -2.56023169e-01 -3.19057167e-01\n",
            "   1.77629933e-01 -9.30741653e-02  3.61943066e-01 -2.11733937e-01\n",
            "  -5.72929740e-01  3.76064360e-01 -4.31921571e-01 -3.16004679e-02\n",
            "   6.86237931e-01  4.75316107e-01 -3.47068995e-01  1.03380211e-01]\n",
            " [-1.76114008e-01 -5.02883732e-01  1.72376961e-01 -5.34390733e-02\n",
            "   2.03963637e-01 -6.66965097e-02  2.71668345e-01  3.33599329e-01\n",
            "  -9.60783511e-02  7.52572417e-02  2.09182829e-01  1.50470570e-01\n",
            "  -3.70911032e-01 -2.59613544e-01 -1.92597762e-01  2.98067838e-01\n",
            "  -2.74568915e-01  9.65832472e-02  1.31773101e-02  1.02015115e-01\n",
            "  -1.18920267e-01  2.34219551e-01  1.21772632e-01 -1.19931027e-01\n",
            "   3.72570932e-01  1.95736811e-01  1.63297802e-01  1.49677768e-01\n",
            "   1.89740852e-01  2.93517590e-01  3.78485531e-01 -3.32200766e-01\n",
            "   7.64950141e-02 -3.15784067e-01 -1.70735508e-01 -9.43595544e-02\n",
            "   2.43606195e-01  2.14079618e-01 -2.34906435e-01 -1.72439933e-01\n",
            "  -7.91637227e-02  8.01218450e-01 -1.10562913e-01  4.74483699e-01\n",
            "  -1.21583352e-02 -1.13443390e-01 -3.42167944e-01  2.13451669e-01\n",
            "   1.61649674e-01 -1.32916356e-02 -3.89783323e-01  5.02630591e-01\n",
            "   1.10692658e-01 -3.12020302e-01 -5.59117496e-01  1.70292169e-01\n",
            "   9.81712863e-02  3.73305261e-01 -1.84545919e-01 -3.04553688e-01\n",
            "  -1.40411362e-01 -4.98259246e-01  2.43641034e-01 -1.97167948e-01\n",
            "   8.02011415e-02 -4.31287616e-01  4.06496860e-02  1.24044437e-02\n",
            "   2.34015076e-03 -8.42931494e-02  4.90815401e-01  3.30242127e-01\n",
            "   4.49791461e-01 -3.86109799e-01 -2.64392402e-02 -4.81144369e-01\n",
            "  -3.50591749e-01 -3.35518271e-01 -2.07767889e-01 -6.05739176e-01\n",
            "  -2.68468648e-01  2.81290263e-01 -1.56250879e-01 -8.78805593e-02\n",
            "  -1.96526900e-01  7.37213418e-02  3.94549578e-01  1.93840459e-01\n",
            "  -5.69188654e-01  3.73506546e-01  6.97457865e-02  3.60685699e-02\n",
            "   1.03936303e+00  3.30224067e-01 -7.58556545e-01  4.63839248e-02]\n",
            " [-2.82666320e-03 -1.98833719e-01 -1.76186383e-01 -2.45463744e-01\n",
            "  -1.23518758e-01  1.17590785e-01  9.42065194e-02  3.57494175e-01\n",
            "  -1.30139701e-02  1.77410260e-01  3.98498088e-01  1.26997292e-01\n",
            "  -3.22956741e-01 -3.56329322e-01 -2.78825074e-01  1.81656182e-01\n",
            "  -4.46840733e-01  1.36728600e-01 -2.88473189e-01  2.65452228e-02\n",
            "  -1.04983091e-01  1.28949299e-01  2.14572057e-01 -2.34644383e-01\n",
            "   3.35539162e-01  1.64284885e-01  1.68361917e-01  4.24401283e-01\n",
            "   5.29036462e-01  1.33461848e-01  3.87158662e-01 -1.86732292e-01\n",
            "   5.00838552e-03 -4.07427371e-01 -3.16643208e-01 -4.99400087e-02\n",
            "   3.44689101e-01  1.76362365e-01 -1.32178918e-01 -3.85579690e-02\n",
            "  -3.31133187e-01  6.29873693e-01  1.98708266e-01  2.83713132e-01\n",
            "   3.59976619e-01  1.15586063e-02 -3.83166432e-01  3.61752868e-01\n",
            "  -9.72529594e-03  6.40185624e-02 -2.83764631e-01  3.53694916e-01\n",
            "   3.05349052e-01 -2.90302277e-01 -3.93050611e-01 -3.02323192e-01\n",
            "   3.93862464e-02  5.48451021e-02 -1.71373770e-01  1.18589327e-01\n",
            "  -1.15610823e-01 -3.52565974e-01  9.73919705e-02 -1.81184381e-01\n",
            "   1.97319031e-01 -1.77318409e-01  3.30584317e-01  2.16132700e-02\n",
            "  -2.83716947e-01 -3.78817230e-01  2.05160677e-01  4.91858602e-01\n",
            "   3.06362331e-01 -2.46464759e-01  5.87063767e-02 -3.20079684e-01\n",
            "  -3.96485984e-01 -5.45371115e-01 -9.27517414e-02 -4.19729501e-01\n",
            "  -2.38988414e-01  1.66863576e-02 -6.91833273e-02 -1.55255646e-01\n",
            "  -5.20176478e-02 -2.74253368e-01  1.39973342e-01 -2.07194954e-01\n",
            "  -4.64913070e-01  1.06281094e-01 -1.20907359e-01  3.91446859e-01\n",
            "   8.57498109e-01  5.21372080e-01 -4.78752345e-01 -2.86330581e-01]\n",
            " [ 7.33427107e-02 -5.10451436e-01  1.44734681e-01  1.54273212e-01\n",
            "  -2.78905272e-01 -3.20937783e-02  3.89437705e-01  6.46809712e-02\n",
            "  -9.18718576e-02  1.51119471e-01  1.91353783e-01 -2.90445447e-01\n",
            "  -2.15330258e-01 -7.05967173e-02 -2.12286234e-01 -2.02242196e-01\n",
            "  -1.27653912e-01 -2.00985909e-01  2.97148138e-01  1.40918598e-01\n",
            "   3.15946043e-01  9.00717750e-02 -1.75179973e-01 -3.44974667e-01\n",
            "   3.01727980e-01  1.25532642e-01  2.49526739e-01  3.21721852e-01\n",
            "   6.32927597e-01  2.39455894e-01  4.71445441e-01  1.15397438e-01\n",
            "   4.51875508e-01 -5.40209711e-01 -1.21303186e-01 -2.15580240e-01\n",
            "   1.53201386e-01 -1.62378639e-01  1.48783967e-01  6.72040761e-01\n",
            "   3.70716415e-02  5.31603515e-01 -1.93686083e-01  3.47551197e-01\n",
            "   2.91548759e-01 -5.37497662e-02 -3.10317546e-01  2.22946107e-01\n",
            "  -2.84830719e-04 -4.28778157e-02 -3.87321860e-01  1.04843333e-01\n",
            "  -3.80794611e-03 -1.80781439e-01  2.06803709e-01  1.84026763e-01\n",
            "  -9.22350492e-03  1.14878602e-01 -1.51499778e-01 -3.97231758e-01\n",
            "  -7.78316930e-02 -2.95292735e-01 -1.47315755e-01  2.51645952e-01\n",
            "   5.25972545e-01 -2.22783625e-01  1.09998487e-01 -5.57259060e-02\n",
            "  -4.07275297e-02  8.85386392e-02  1.21189632e-01  2.69644082e-01\n",
            "   9.04884413e-02 -2.18156129e-01 -2.14687377e-01 -4.05903518e-01\n",
            "  -9.88801643e-02 -6.31533742e-01  2.53991246e-01 -7.61697948e-01\n",
            "  -1.17290147e-01 -3.32612991e-01 -4.74842250e-01 -3.36059958e-01\n",
            "   2.49061614e-01  3.51076752e-01  3.65727171e-02  3.68432142e-02\n",
            "  -6.32921517e-01 -1.14078959e-02 -4.08907421e-02 -4.62007783e-02\n",
            "   5.59540153e-01  3.12620819e-01 -5.44424951e-01  2.58681089e-01]]\n",
            "\n",
            "Named Entity Recognition (NER) Features:\n",
            "[[], [], ['ORG'], ['DATE'], []]\n",
            "\n",
            "Part-of-Speech (POS) Features:\n",
            "[['DET', 'ADJ', 'NOUN', 'NOUN', 'VERB', 'PART', 'VERB', 'NOUN', 'ADP', 'ADJ', 'NOUN', 'ADP', 'DET', 'NOUN', 'PUNCT'], ['DET', 'NOUN', 'NOUN', 'VERB', 'PRON', 'NOUN', 'ADP', 'DET', 'NOUN', 'ADP', 'DET', 'NOUN', 'NOUN', 'PUNCT'], ['PROPN', 'PROPN', 'VERB', 'PRON', 'ADJ', 'PROPN', 'NOUN', 'ADP', 'DET', 'ADJ', 'NOUN', 'NOUN', 'NOUN', 'PUNCT'], ['DET', 'ADJ', 'NOUN', 'NOUN', 'VERB', 'PROPN', 'NOUN', 'NOUN', 'ADP', 'PRON', 'NOUN', 'NOUN', 'PUNCT'], ['DET', 'ADJ', 'NOUN', 'VERB', 'SCONJ', 'ADJ', 'NOUN', 'AUX', 'VERB', 'DET', 'NOUN', 'ADP', 'NOUN', 'NOUN', 'PUNCT']]\n",
            "\n",
            "Sentiment Analysis Features:\n",
            "[0.06818181818181818, 0.04999999999999999, 0.5, 0.5, 0.06818181818181818]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "import spacy\n",
        "from textblob import TextBlob\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "\n",
        "# Sample news articles\n",
        "news_articles = [\n",
        "   \"The new healthcare policy aims to improve access to medical services for all citizens.\",\n",
        "   \"The football team celebrated their victory with a parade through the city streets.\",\n",
        "   \"Apple Inc. unveiled its latest iPhone model at the annual product launch event.\",\n",
        "   \"The latest movie blockbuster broke box office records on its opening weekend.\",\n",
        "   \"A new study suggests that regular exercise can reduce the risk of heart disease.\"\n",
        "]\n",
        "\n",
        "# Initializing spaCy model for Named Entity Recognition (NER) and Part-of-Speech (POS) tagging\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Function to perform Bag-of-Words (BoW) feature extraction\n",
        "def bow_features(articles):\n",
        "    # Initialize CountVectorizer\n",
        "    vectorizer = CountVectorizer()\n",
        "    # Fit and transform the articles to BoW matrix\n",
        "    bow_matrix = vectorizer.fit_transform(articles)\n",
        "    # Get feature names\n",
        "    feature_names = vectorizer.get_feature_names_out()\n",
        "    return bow_matrix, feature_names\n",
        "\n",
        "# Function to perform word embeddings (Word2Vec)\n",
        "def word_embeddings(articles):\n",
        "    embeddings = []\n",
        "    # Generating word embeddings for each article\n",
        "    for article in articles:\n",
        "        # Tokenizing the article and generate word embeddings using spaCy\n",
        "        doc = nlp(article)\n",
        "        # Computing the mean of word embeddings for the article\n",
        "        article_embedding = np.mean([token.vector for token in doc], axis=0)\n",
        "        embeddings.append(article_embedding)\n",
        "    return np.array(embeddings)\n",
        "\n",
        "# Function to perform Named Entity Recognition (NER) feature extraction\n",
        "def ner_features(articles):\n",
        "    ner_tags = []\n",
        "    # Extracting NER tags for each article\n",
        "    for article in articles:\n",
        "        # Processing the article using spaCy\n",
        "        doc = nlp(article)\n",
        "        # Extracting NER tags and append to the list\n",
        "        ner_tags.append([ent.label_ for ent in doc.ents])\n",
        "    return ner_tags\n",
        "\n",
        "# Function to perform Part-of-Speech (POS) feature extraction\n",
        "def pos_features(articles):\n",
        "    pos_tags = []\n",
        "    # Extracting POS tags for each article\n",
        "    for article in articles:\n",
        "        # Processing the article using spaCy\n",
        "        doc = nlp(article)\n",
        "        # Extracting POS tags and append to the list\n",
        "        pos_tags.append([token.pos_ for token in doc])\n",
        "    return pos_tags\n",
        "\n",
        "# Function to perform sentiment analysis feature extraction\n",
        "def sentiment_features(articles):\n",
        "    sentiment_scores = []\n",
        "    # Computing sentiment polarity score for each article\n",
        "    for article in articles:\n",
        "        # Using TextBlob to compute sentiment polarity\n",
        "        blob = TextBlob(article)\n",
        "        sentiment_scores.append(blob.sentiment.polarity)\n",
        "    return sentiment_scores\n",
        "\n",
        "# Performing feature extraction\n",
        "bow_matrix, bow_feature_names = bow_features(news_articles)\n",
        "word_embeddings_matrix = word_embeddings(news_articles)\n",
        "ner_tags = ner_features(news_articles)\n",
        "pos_tags = pos_features(news_articles)\n",
        "sentiment_scores = sentiment_features(news_articles)\n",
        "\n",
        "# Printing the extracted features\n",
        "print(\"Bag-of-Words Features:\")\n",
        "print(bow_matrix.toarray())\n",
        "print(\"Feature Names:\",bow_feature_names)\n",
        "print(\"\\nWord Embeddings:\")\n",
        "print(word_embeddings_matrix)\n",
        "print(\"\\nNamed Entity Recognition (NER) Features:\")\n",
        "print(ner_tags)\n",
        "print(\"\\nPart-of-Speech (POS) Features:\")\n",
        "print(pos_tags)\n",
        "print(\"\\nSentiment Analysis Features:\")\n",
        "print(sentiment_scores)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7oSK4soH70nf"
      },
      "source": [
        "## Question 3 (10 points):\n",
        "Use any of the feature selection methods mentioned in this paper \"Deng, X., Li, Y., Weng, J., & Zhang, J. (2019). Feature selection for text classification: A review. Multimedia Tools & Applications, 78(3).\"\n",
        "\n",
        "Select the most important features you extracted above, rank the features based on their importance in the descending order."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_selection import SelectKBest, chi2\n",
        "\n",
        "#Class-dependent feature selection\n",
        "\n",
        "#Labels corresponding to each sentence are defined. These labels represent the categories to which each sentence belongs\n",
        "labels = ['medical', 'sports', 'technology', 'entertainment', 'medical']\n",
        "\n",
        "# A feature matrix is created where each row represents a sentence and each column represents a word in the vocabulary.\n",
        "feature_matrix = np.zeros((len(sentences), len(vocabulary)), dtype=int)\n",
        "\n",
        "for i, sentence_words in enumerate(words):\n",
        "    for j, word in enumerate(vocabulary):\n",
        "        if word in sentence_words:\n",
        "            feature_matrix[i, j] = 1\n",
        "\n",
        "# The SelectKBest method is used to perform class-dependent feature selection based on the chi-squared test. The chi2 function is passed as the scoring function.\n",
        "k_best = SelectKBest(score_func=chi2, k=10)  # The fit_transform method of SelectKBest is used to fit the feature matrix to the labels and select the top k features (in this case, top 10 features\n",
        "selected_features = k_best.fit_transform(feature_matrix, labels)\n",
        "\n",
        "# The get_support(indices=True) method is used to get the indices of the selected features.\n",
        "sf_indices = k_best.get_support(indices=True)\n",
        "\n",
        "# Get the names of selected features\n",
        "sf_names = [list(vocabulary)[i] for i in sf_indices]\n",
        "print(sf_names)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZuJPC6bhJaWH",
        "outputId": "dae1538b-820c-4ede-86e3-25b1d1d1f331"
      },
      "execution_count": 139,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['team', 'records', 'through', 'celebrated', 'product', 'annual', 'blockbuster', 'event.', 'iphone', 'city']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Ranking\n",
        "# The scores_ attribute of the SelectKBest object (k_best) contains the chi-squared scores for each feature.\n",
        "chi2Scores = k_best.scores_\n",
        "\n",
        "# Creating a dictionary to store feature importance scores for each selected feature\n",
        "feature_importance_dict = {feature_name: chi2_score for feature_name, chi2_score in zip(sf_names, chi2Scores)}\n",
        "\n",
        "# Sorting features based on importance in descending order\n",
        "desc_features = sorted(feature_importance_dict.items(), key=lambda x: x[1], reverse=True)\n",
        "\n",
        "# Printing the ranked features\n",
        "print(\"Ranked selected features based on importance:\")\n",
        "for i, (feature_name, importance_score) in enumerate(desc_features):\n",
        "    print(f\"{i+1}. {feature_name}: {importance_score:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "84S89WGaMv4h",
        "outputId": "d46a438d-cab5-4012-81e9-a40470589453"
      },
      "execution_count": 138,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ranked selected features based on importance:\n",
            "1. through: 4.0000\n",
            "2. celebrated: 4.0000\n",
            "3. product: 4.0000\n",
            "4. event.: 4.0000\n",
            "5. iphone: 4.0000\n",
            "6. blockbuster: 1.7500\n",
            "7. team: 1.5000\n",
            "8. records: 1.5000\n",
            "9. annual: 1.5000\n",
            "10. city: 1.5000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7nZGAOwl70ng"
      },
      "source": [
        "## Question 4 (10 points):\n",
        "Write python code to rank the text based on text similarity. Based on the text data you used for question 2, design a query to match the most relevant docments. Please use the BERT model to represent both your query and the text data, then calculate the cosine similarity between the query and each text in your data. Rank the similary with descending order."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 141,
      "metadata": {
        "id": "b4HoWK-i70ng",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1f33037a-ea6c-4afa-9d29-e7151c17404b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Similarity: 0.5618 | Article: The new healthcare policy aims to improve access to medical services for all citizens.\n",
            "Similarity: 0.4503 | Article: A new study suggests that regular exercise can reduce the risk of heart disease.\n",
            "Similarity: 0.4468 | Article: The football team celebrated their victory with a parade through the city streets.\n",
            "Similarity: 0.4413 | Article: Apple Inc. unveiled its latest iPhone model at the annual product launch event.\n",
            "Similarity: 0.3614 | Article: The latest movie blockbuster broke box office records on its opening weekend.\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from transformers import BertTokenizer, BertModel\n",
        "\n",
        "# Sample news articles\n",
        "news_articles = [\n",
        "   \"The new healthcare policy aims to improve access to medical services for all citizens.\",\n",
        "   \"The football team celebrated their victory with a parade through the city streets.\",\n",
        "   \"Apple Inc. unveiled its latest iPhone model at the annual product launch event.\",\n",
        "   \"The latest movie blockbuster broke box office records on its opening weekend.\",\n",
        "   \"A new study suggests that regular exercise can reduce the risk of heart disease.\"\n",
        "]\n",
        "\n",
        "# The BERT tokenizer and model are initialized using the bert-base-uncased pre-trained model.\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "model = BertModel.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# The encode_text function takes an input text, tokenizes it using the BERT tokenizer, and then passes it through the BERT model to obtain contextual embeddings.\n",
        "def encode_text(text):\n",
        "    tokens = tokenizer(text, padding=True, truncation=True, return_tensors='pt')\n",
        "    outputs = model(**tokens)\n",
        "    embeddings = outputs.last_hidden_state.mean(dim=1)  #mean pooling of token embeddings is used to obtain a fixed-size representation for the text. The embeddings are then converted to NumPy arrays and returned\n",
        "    return embeddings.detach().numpy()\n",
        "\n",
        "# Encode query and text data using BERT\n",
        "query = \"healthcare policy\"        #Each news article in the news_articles list is also encoded using the same function. The resulting embeddings are stored in NumPy arrays.\n",
        "query_encoding = encode_text(query)\n",
        "txt_encodings = np.array([encode_text(text) for text in news_articles])\n",
        "\n",
        "# The query encoding and text encodings are reshaped to ensure they have the correct dimensions for cosine similarity calculation.\n",
        "query_encoding = query_encoding.reshape(1, -1) if len(query_encoding.shape) > 1 else query_encoding\n",
        "\n",
        "# Reshape the text encodings to ensure they are 2D\n",
        "txt_encodings = np.squeeze(txt_encodings)\n",
        "txt_encodings = txt_encodings.reshape(txt_encodings.shape[0], -1) if len(txt_encodings.shape) > 2 else txt_encodings\n",
        "\n",
        "\n",
        "# Calculating cosine similarity between query and each text\n",
        "similarities = cosine_similarity(query_encoding, txt_encodings)\n",
        "\n",
        "# Ranking the similarities in descending order\n",
        "sorted_indices = similarities.argsort()[0][::-1]\n",
        "sorted_similarities = similarities[0, sorted_indices]\n",
        "\n",
        "# Printing the ranked similarities and corresponding articles\n",
        "for idx, sim in zip(sorted_indices, sorted_similarities):\n",
        "    print(f\"Similarity: {sim:.4f} | Article: {news_articles[idx]}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Mandatory Question"
      ],
      "metadata": {
        "id": "VEs-OoDEhTW4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Important: Reflective Feedback on this exercise**\n",
        "\n",
        "Please provide your thoughts and feedback on the exercises you completed in this assignment. Consider the following points in your response:\n",
        "\n",
        "Learning Experience: Describe your overall learning experience in working on extracting features from text data. What were the key concepts or techniques you found most beneficial in understanding the process?\n",
        "\n",
        "Challenges Encountered: Were there specific difficulties in completing this exercise?\n",
        "\n",
        "Relevance to Your Field of Study: How does this exercise relate to the field of NLP?\n",
        "\n",
        "**(Your submission will not be graded if this question is left unanswered)**\n",
        "\n"
      ],
      "metadata": {
        "id": "IUKC7suYhVl0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Your answer here (no code for this question, write down your answer as detail as possible for the above questions):\n",
        "\n",
        "'''\n",
        " It was a valuable learning experience overall.Understanding various extraction techniques and trying to understand them and relating to real-time problems was exciting.One of the main challenges was ensuring data consistency and compatibility across different preprocessing and feature extraction steps.\n",
        "  Handling unseen labels or unexpected data values during preprocessing and encoding also required careful attention and troubleshooting.Given time was not sufficient to deep dive into the concepts to write the code on own . Understanding the input format for question:3 was challenging and did not have clarity on the expected output.\n",
        "This exercise in feature extraction from text data is highly relevant to Information Science. It enables the extraction of meaningful features from textual information, facilitating tasks such as document classification, information retrieval, and semantic analysis.\n",
        "\n",
        "'''"
      ],
      "metadata": {
        "id": "CAq0DZWAhU9m"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}