{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NagillaUdayasree/Udayasree_INFO5731_Spring2024/blob/main/Nagilla_Udayasree_Assignment_Four.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "USSdXHuqnwv9"
      },
      "source": [
        "# **INFO5731 Assignment Four**\n",
        "\n",
        "In this assignment, you are required to conduct topic modeling, sentiment analysis based on **the dataset you created from assignment three**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YWxodXh5n4xF"
      },
      "source": [
        "# **Question 1: Topic Modeling**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TenBkDJ5n95k"
      },
      "source": [
        "(30 points). This question is designed to help you develop a feel for the way topic modeling works, the connection to the human meanings of documents. Based on the dataset from assignment three, write a python program to **identify the top 10 topics in the dataset**. Before answering this question, please review the materials in lesson 8, especially the code for LDA, LSA, and BERTopic. The following information should be reported:\n",
        "\n",
        "1. Features (text representation) used for topic modeling.\n",
        "\n",
        "2. Top 10 clusters for topic modeling.\n",
        "\n",
        "3. Summarize and describe the topic for each cluster.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RqgScHj5EJ9r",
        "outputId": "92586d5b-5819-4820-9404-511e93202d50"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "WARNING:gensim.models.ldamulticore:too few updates, training might not converge; consider increasing the number of passes or iterations to improve accuracy\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "********************************************************************************\n",
            "Top 10 topics:\n",
            "Topic 0: 0.030*\"film\" + 0.016*\"movie\" + 0.012*\"prison\" + 0.011*\"andy\" + 0.010*\"shawshank\" + 0.008*\"one\" + 0.008*\"redemption\" + 0.007*\"life\" + 0.007*\"freeman\" + 0.006*\"time\"\n",
            "Topic 1: 0.033*\"movie\" + 0.017*\"film\" + 0.010*\"prison\" + 0.010*\"shawshank\" + 0.009*\"time\" + 0.009*\"one\" + 0.009*\"story\" + 0.007*\"life\" + 0.007*\"redemption\" + 0.006*\"freeman\"\n",
            "Topic 2: 0.022*\"movie\" + 0.017*\"film\" + 0.013*\"prison\" + 0.013*\"andy\" + 0.009*\"shawshank\" + 0.009*\"one\" + 0.009*\"time\" + 0.007*\"hope\" + 0.007*\"life\" + 0.007*\"redemption\"\n",
            "Topic 3: 0.024*\"movie\" + 0.010*\"film\" + 0.007*\"shawshank\" + 0.007*\"prison\" + 0.006*\"redemption\" + 0.006*\"like\" + 0.006*\"andy\" + 0.005*\"freeman\" + 0.005*\"best\" + 0.005*\"good\"\n",
            "Topic 4: 0.049*\"movie\" + 0.013*\"one\" + 0.011*\"best\" + 0.010*\"film\" + 0.009*\"prison\" + 0.009*\"time\" + 0.009*\"hope\" + 0.008*\"story\" + 0.007*\"shawshank\" + 0.007*\"good\"\n",
            "Topic 5: 0.019*\"film\" + 0.018*\"movie\" + 0.011*\"time\" + 0.010*\"shawshank\" + 0.007*\"story\" + 0.007*\"one\" + 0.007*\"prison\" + 0.006*\"freeman\" + 0.006*\"best\" + 0.006*\"see\"\n",
            "Topic 6: 0.015*\"film\" + 0.013*\"movie\" + 0.009*\"one\" + 0.008*\"andy\" + 0.008*\"shawshank\" + 0.007*\"prison\" + 0.007*\"story\" + 0.006*\"freeman\" + 0.006*\"life\" + 0.006*\"time\"\n",
            "Topic 7: 0.019*\"film\" + 0.018*\"movie\" + 0.013*\"time\" + 0.009*\"prison\" + 0.008*\"story\" + 0.007*\"shawshank\" + 0.007*\"good\" + 0.006*\"redemption\" + 0.006*\"one\" + 0.005*\"make\"\n",
            "Topic 8: 0.033*\"film\" + 0.015*\"shawshank\" + 0.013*\"movie\" + 0.010*\"one\" + 0.010*\"redemption\" + 0.008*\"prison\" + 0.008*\"best\" + 0.007*\"time\" + 0.007*\"freeman\" + 0.007*\"robbins\"\n",
            "Topic 9: 0.022*\"movie\" + 0.013*\"film\" + 0.011*\"prison\" + 0.011*\"one\" + 0.009*\"shawshank\" + 0.009*\"time\" + 0.008*\"andy\" + 0.008*\"hope\" + 0.008*\"story\" + 0.007*\"redemption\"\n",
            "********************************************************************************\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd  # pandas library for data manipulation and analysis\n",
        "import gensim  # gensim library for topic modeling\n",
        "from gensim import corpora  # corpora module from gensim for creating dictionaries and corpus\n",
        "import nltk  # nltk library for natural language processing tasks\n",
        "nltk.download('punkt')  #  the 'punkt' resource for tokenization\n",
        "nltk.download('stopwords')  #  the 'stopwords' corpus for removing stopwords\n",
        "nltk.download('wordnet')  #  the 'wordnet' corpus for lemmatization\n",
        "from nltk.corpus import stopwords  #  stopwords corpus from nltk\n",
        "from nltk.stem import WordNetLemmatizer  #  the WordNetLemmatizer class from nltk for lemmatization\n",
        "\n",
        "# Load the dataset\n",
        "data = pd.read_csv('classified_sentiment_output.csv')  # Read the CSV file into a pandas DataFrame\n",
        "\n",
        "# Preprocessing functions\n",
        "def tokenize_text(txt):\n",
        "    return nltk.word_tokenize(txt.lower())  # Tokenize the text and convert to lowercase\n",
        "\n",
        "def remove_stopwords(tokens):\n",
        "    stop_words = set(stopwords.words('english'))  # Create a set of English stopwords\n",
        "    return [word for word in tokens if word not in stop_words]  # Remove stopwords from the token list\n",
        "\n",
        "def lemmatize_tokens(tokens):\n",
        "    lemmatizer = WordNetLemmatizer()  # Create an instance of the WordNetLemmatizer\n",
        "    return [lemmatizer.lemmatize(word) for word in tokens]  # Lemmatize each token in the list\n",
        "\n",
        "def preprocess(text):\n",
        "    tokens = tokenize_text(text)  # Tokenize the text\n",
        "    tokens = remove_stopwords(tokens)  # Remove stopwords from the tokens\n",
        "    tokens = lemmatize_tokens(tokens)  # Lemmatize the tokens\n",
        "    return tokens  # Return the preprocessed tokens\n",
        "\n",
        "# Preprocess the text data\n",
        "processed_texts = [preprocess(text) for text in data['clean_text']]  # Apply the preprocess function to each text in the 'clean_text' column\n",
        "\n",
        "# Create a dictionary from the processed texts\n",
        "dictionary = corpora.Dictionary(processed_texts)  # Create a dictionary from the processed texts\n",
        "\n",
        "# Convert the texts to a document-term matrix\n",
        "corpus = [dictionary.doc2bow(text) for text in processed_texts]  # Convert the processed texts to a document-term matrix\n",
        "\n",
        "# Create an LDA model\n",
        "num_topics = 10  # Number of topics to extract\n",
        "lda_model = gensim.models.LdaMulticore(corpus=corpus, id2word=dictionary, num_topics=num_topics)  # Create an LDA model with 10 topics\n",
        "\n",
        "print(\"*\" * 80)  # Print a separator line\n",
        "# Print the top topics\n",
        "print(\"Top {} topics:\".format(num_topics))  # Print a header for the top topics\n",
        "for idx, topic in lda_model.print_topics(-1):  # Iterate over the top topics\n",
        "    print(\"Topic {}: {}\".format(idx, topic))  # Print the topic index and keywords\n",
        "print(\"*\" * 80)  # Print a separator line"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d66XstWiKlhs"
      },
      "outputs": [],
      "source": [
        "\"\"\"Topic modeling using the Natural Language Toolkit (NLTK) and Gensim, and the resulting output of the top 10 topics identified from an LDA model.\n",
        "\n",
        "### Features (Text Representation) Used for Topic Modeling\n",
        "\n",
        "- Tokenization: This breaks text into individual words using NLTK's tokenizer, ensuring that the text is processed at a word level.\n",
        "- Case Normalization: All text is converted to lowercase to ensure that the same words in different cases are counted as the same word.\n",
        "- Stop Word Removal: Common words that generally do not contribute to the meaning of the text, such as \"the\", \"is\", and \"and\", are removed using a predefined list from NLTK.\n",
        "- Lemmatization: Words are reduced to their base or root form. This process is more sophisticated than stemming as it uses lexical knowledge bases to get the correct base forms of words.\n",
        "- Vectorization (Document-Term Matrix): Using the Bag of Words model, texts are converted into a numerical format where each unique word is represented by an integer and each document is a vector of word counts.\n",
        "\n",
        "### Top 10 Clusters for Topic Modeling\n",
        "\n",
        "The LDA model identified the following 10 topics as clusters of words that frequently co-occur across the documents:\n",
        "\n",
        "Topic 0: Primarily related to movies and films, with a focus on \"prison\" themes.\n",
        "Topic 1: Focuses on general movie discussions, highlighting the viewing experience (\"time\", \"story\").\n",
        "Topic 2: Combines elements of movies and prison settings with emotional themes (\"hope\", \"life\").\n",
        "Topic 3: A mixture of film critique and specific elements from \"Shawshank Redemption\" (like \"redemption\").\n",
        "Topic 4: Revolves around the \"best\" aspects of movies, possibly award discussions or top movie lists.\n",
        "Topic 5: Discusses narrative elements in films, including storytelling and plot summaries.\n",
        "Topic 6: Encompasses a variety of film elements but seems to focus on personal stories within movies.\n",
        "Topic 7: Appears to discuss film impact and narrative quality, possibly in reviews.\n",
        "Topic 8: Heavy on specific references to \"Shawshank Redemption\", discussing characters and plot details.\n",
        "Topic 9: Mixes general movie discussion with focus on prison life and philosophical themes like \"hope\".\n",
        "\n",
        "### Summarize and Describe the Topic for Each Cluster\n",
        "\n",
        "1. Topic 0 - (Prison Drama Insights): Explores dramatic narratives set in prison environments, specifically highlighting \"Shawshank Redemption\" and key characters like Andy and themes of life and time.\n",
        "\n",
        "2. Topic 1 - (Cinematic Timing and Storytelling): Delves into how movies manage storytelling within their runtime, with frequent references to narrative pacing and life reflections.\n",
        "\n",
        "3. Topic 2 - (Hope and Redemption in Film): Focuses on the themes of hope and redemption within the context of prison movies, likely drawing heavily from \"Shawshank Redemption\".\n",
        "\n",
        "4.Topic 3 - (Critical Reception and Values): Discusses what makes movies like \"Shawshank Redemption\" critically acclaimed, touching on themes like redemption and performance.\n",
        "\n",
        "5. Topic 4 - (Celebrating Cinematic Excellence): Centers on movies considered the \"best\", often discussing their hopeful messages and timeless stories.\n",
        "\n",
        "6. Topic 5 - (Narrative and Plot Analysis): Analyzes the storytelling techniques and plot elements in movies, with a focus on how stories are seen and appreciated.\n",
        "\n",
        "7. Topic 6 - (Personal and Emotional Film Narratives): Looks at personal and emotional aspects of film stories, particularly how characters like Andy influence the narrative.\n",
        "\n",
        "8. Topic 7 - (Reviewing Film Impact): Reviews the impact of films on audiences, considering how stories are made and received in terms of quality and emotional engagement.\n",
        "\n",
        "9. Topic 8 - (Character Deep Dives): Provides deep dives into specific characters and themes from \"Shawshank Redemption\", discussing their role in the film's success.\n",
        "\n",
        "10. Topic 9 - (Philosophical and Life Themes in Movies): Explores philosophical questions and life themes presented in films, especially in prison settings, with a focus on hope and redemption narratives.\n",
        "\n",
        "Each topic summary provides insight into the different aspects of how films, particularly \"Shawshank Redemption\", are discussed and analyzed.\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AfpMRCrRwN6Z"
      },
      "source": [
        "# **Question 2: Sentiment Analysis**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1dCQEbDawWCw"
      },
      "source": [
        "(30 points). Sentiment analysis also known as opinion mining is a sub field within Natural Language Processing (NLP) that builds machine learning algorithms to classify a text according to the sentimental polarities of opinions it contains, e.g., positive, negative, neutral. The purpose of this question is to develop a machine learning classifier for sentiment analysis. Based on the dataset from assignment three, write a python program to implement a sentiment classifier and evaluate its performance. Notice: **80% data for training and 20% data for testing**.  \n",
        "\n",
        "1. Select features for the sentiment classification and explain why you select these features. Use a markdown cell to provide your explanation.\n",
        "\n",
        "2. Select two of the supervised learning algorithms/models from scikit-learn library: https://scikit-learn.org/stable/supervised_learning.html#supervised-learning, to build two sentiment classifiers respectively. Note: Cross-validation (5-fold or 10-fold) should be conducted. Here is the reference of cross-validation: https://scikit-learn.org/stable/modules/cross_validation.html.\n",
        "\n",
        "3. Compare the performance over accuracy, precision, recall, and F1 score for the two algorithms you selected. The test set must be used for model evaluation in this step. Here is the reference of how to calculate these metrics: https://towardsdatascience.com/accuracy-precision-recall-or-f1-331fb37c5cb9."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vATjQNTY8buA",
        "outputId": "587bc10a-51a2-40a3-9ea1-04f1a177c743"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Naive Bayes Classifier\n",
            "Accuracy: 0.8390\n",
            "Precision: 0.7040\n",
            "Recall: 0.8390\n",
            "F1-score: 0.7656\n",
            "Cross-validation scores: [0.81707317 0.81707317 0.81595092 0.81595092 0.81595092]\n",
            "Mean cross-validation score: 0.8164\n",
            "\n",
            "Decision Tree Classifier\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 0.7366\n",
            "Precision: 0.7518\n",
            "Recall: 0.7366\n",
            "F1-score: 0.7437\n",
            "Cross-validation scores: [0.74390244 0.76219512 0.79754601 0.77300613 0.79141104]\n",
            "Mean cross-validation score: 0.7736\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd  #pandas library for data manipulation and analysis\n",
        "from sklearn.model_selection import train_test_split  # train_test_split function for splitting the data into training and testing sets\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer  #TfidfVectorizer for creating TF-IDF features\n",
        "from sklearn.naive_bayes import MultinomialNB  # MultinomialNB classifier from scikit-learn\n",
        "from sklearn.tree import DecisionTreeClassifier  #DecisionTreeClassifier from scikit-learn\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score  #evaluation metrics from scikit-learn\n",
        "from sklearn.model_selection import cross_val_score  #cross_val_score function for cross-validation\n",
        "\n",
        "# Load the dataset\n",
        "sentiment_data = pd.read_csv('classified_sentiment_output.csv')\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "text_data = sentiment_data['clean_text']  # Extract the 'clean_text' column from the DataFrame\n",
        "labels = sentiment_data['Sentiment']  # Extract the 'Sentiment' column from the DataFrame\n",
        "train_text, test_text, train_labels, test_labels = train_test_split(text_data, labels, test_size=0.2, random_state=45)\n",
        "# Split the data into training and testing sets (80% for training, 20% for testing) with a fixed random state\n",
        "\n",
        "# TF-IDF features\n",
        "vectorizer = TfidfVectorizer()  # Creating an instance of the TfidfVectorizer\n",
        "train_features = vectorizer.fit_transform(train_text)  # Creating TF-IDF features for the training text data\n",
        "test_features = vectorizer.transform(test_text)  # Creating TF-IDF features for the testing text data\n",
        "\n",
        "# Naive Bayes Classifier\n",
        "print(\"Naive Bayes Classifier\")  # header for the Naive Bayes Classifier\n",
        "nb_classifier = MultinomialNB()  # Creating an instance of the MultinomialNB classifier\n",
        "nb_classifier.fit(train_features, train_labels)  # Training the Naive Bayes Classifier on the training data\n",
        "nb_predictions = nb_classifier.predict(test_features)  # Making predictions on the testing data\n",
        "nb_accuracy = accuracy_score(test_labels, nb_predictions)  # Calculating the accuracy score\n",
        "nb_precision = precision_score(test_labels, nb_predictions, average='weighted')  # Calculating the weighted precision score\n",
        "nb_recall = recall_score(test_labels, nb_predictions, average='weighted')  # Calculating the weighted recall score\n",
        "nb_f1 = f1_score(test_labels, nb_predictions, average='weighted')  # Calculating the weighted F1 score\n",
        "print(f\"Accuracy: {nb_accuracy:.4f}\")  # Printing the accuracy score\n",
        "print(f\"Precision: {nb_precision:.4f}\")  # Printing the precision score\n",
        "print(f\"Recall: {nb_recall:.4f}\")  # Printing the recall score\n",
        "print(f\"F1-score: {nb_f1:.4f}\")  # Printing the F1 score\n",
        "nb_cv_scores = cross_val_score(nb_classifier, train_features, train_labels, cv=5, scoring='accuracy')\n",
        "# Performing 5-fold cross-validation and calculate the accuracy scores for each fold\n",
        "print(f\"Cross-validation scores: {nb_cv_scores}\")  # Printing the cross-validation scores\n",
        "print(f\"Mean cross-validation score: {nb_cv_scores.mean():.4f}\")  # Printing the mean cross-validation score\n",
        "\n",
        "# Decision Tree Classifier\n",
        "print(\"\\nDecision Tree Classifier\")  # header for the Decision Tree Classifier\n",
        "dt_classifier = DecisionTreeClassifier()  # Creating an instance of the DecisionTreeClassifier\n",
        "dt_classifier.fit(train_features, train_labels)  # Training the Decision Tree Classifier on the training data\n",
        "dt_predictions = dt_classifier.predict(test_features)  # Making predictions on the testing data\n",
        "dt_accuracy = accuracy_score(test_labels, dt_predictions)  # Calculating the accuracy score\n",
        "dt_precision = precision_score(test_labels, dt_predictions, average='weighted')  # Calculating the weighted precision score\n",
        "dt_recall = recall_score(test_labels, dt_predictions, average='weighted')  # Calculating the weighted recall score\n",
        "dt_f1 = f1_score(test_labels, dt_predictions, average='weighted')  # Calculating the weighted F1 score\n",
        "print(f\"Accuracy: {dt_accuracy:.4f}\")  # Printing the accuracy score\n",
        "print(f\"Precision: {dt_precision:.4f}\")  # Printing the precision score\n",
        "print(f\"Recall: {dt_recall:.4f}\")  # Printing the recall score\n",
        "print(f\"F1-score: {dt_f1:.4f}\")  # Printing the F1 score\n",
        "dt_cv_scores = cross_val_score(dt_classifier, train_features, train_labels, cv=5, scoring='accuracy')\n",
        "# Performing 5-fold cross-validation and calculate the accuracy scores for each fold\n",
        "print(f\"Cross-validation scores: {dt_cv_scores}\")  # Printing the cross-validation scores\n",
        "print(f\"Mean cross-validation score: {dt_cv_scores.mean():.4f}\")  # Printing the mean cross-validation score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZQpAuWfgXPtX"
      },
      "outputs": [],
      "source": [
        "\"\"\"In the sentiment classification, TF-IDF (Term Frequency-Inverse Document Frequency) is used as the feature extraction method because of the following reasons:\n",
        "\n",
        "TF-IDF as Feature Extraction Method:\n",
        "  Emphasizes Important Words: TF-IDF highlights crucial words for sentiment analysis and diminishes the impact of common, less informative words.\n",
        "  Adjusts Word Importance: It increases the weight of rare words across documents, focusing on terms that uniquely define sentiment in specific texts.\n",
        "\n",
        "Compatibility with Classifiers:\n",
        "  Naive Bayes: TF-IDF aligns with the probabilistic nature of Naive Bayes, treating scores as normalized frequency counts which enhances model suitability.\n",
        "  Decision Trees: Utilizes the numerical vectors from TF-IDF to make decisions based on quantitative thresholds, effectively distinguishing between different sentiment classes.\n",
        "\n",
        "Benefits of Using TF-IDF:\n",
        "  Reduces Computational Complexity: Lowers the dimensionality of the feature space, making data processing more manageable.\n",
        "  Improves Model Performance: Enhances key metrics such as accuracy, precision, recall, and F1-score.\n",
        "  Stable and Reliable Predictions: Cross-validation with TF-IDF features leads to more consistent and dependable outcomes, confirming the effectiveness and efficiency of the models.\n",
        "\n",
        "Performance comparison:\n",
        "  When used for sentiment classification tasks, the Naive Bayes classifier beats the Decision Tree in most important performance criteria. In contrast to the Decision Tree, Naive Bayes exhibits greater accuracy (83.90% vs. 73.66%), recall (83.90% vs. 73.66%), F1-score (76.56% vs. 74.37%), and a mean cross-validation score (81.64% vs. 77.36%) that is more stable. In some areas, the Decision Tree performs worse than the other, but its precision is slightly greater (75.18% vs. 70.40%). For particular labels, the Decision Tree may also have problems with undefinable precision, which could be caused by an imbalance in the classes represented in the predictions or by missing classes. Because Naive Bayes provides balanced and robust performance—especially in terms of detecting all pertinent occurrences without missing positive cases—it is therefore a more dependable option for this dataset.\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E5mmYIfN8eYV"
      },
      "source": [
        "# **Question 3: House price prediction**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hsi2y4z88ngX"
      },
      "source": [
        "(20 points). You are required to build a **regression** model to predict the house price with 79 explanatory variables describing (almost) every aspect of residential homes. The purpose of this question is to practice regression analysis, an supervised learning model. The training data, testing data, and data description files can be download from canvas. Here is an axample for implementation: https://towardsdatascience.com/linear-regression-in-python-predict-the-bay-areas-home-price-5c91c8378878.\n",
        "\n",
        "1. Conduct necessary Explatory Data Analysis (EDA) and data cleaning steps on the given dataset. Split data for training and testing.\n",
        "2. Based on the EDA results, select a number of features for the regression model. Shortly explain why you select those features.\n",
        "3. Develop a regression model. The train set should be used.\n",
        "4. Evaluate performance of the regression model you developed using appropriate evaluation metrics. The test set should be used."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DLp7ojTLKlQD",
        "outputId": "76a38be0-195b-42b8-ea5e-3518573174ee"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 1460 entries, 0 to 1459\n",
            "Data columns (total 81 columns):\n",
            " #   Column         Non-Null Count  Dtype  \n",
            "---  ------         --------------  -----  \n",
            " 0   Id             1460 non-null   int64  \n",
            " 1   MSSubClass     1460 non-null   int64  \n",
            " 2   MSZoning       1460 non-null   object \n",
            " 3   LotFrontage    1201 non-null   float64\n",
            " 4   LotArea        1460 non-null   int64  \n",
            " 5   Street         1460 non-null   object \n",
            " 6   Alley          91 non-null     object \n",
            " 7   LotShape       1460 non-null   object \n",
            " 8   LandContour    1460 non-null   object \n",
            " 9   Utilities      1460 non-null   object \n",
            " 10  LotConfig      1460 non-null   object \n",
            " 11  LandSlope      1460 non-null   object \n",
            " 12  Neighborhood   1460 non-null   object \n",
            " 13  Condition1     1460 non-null   object \n",
            " 14  Condition2     1460 non-null   object \n",
            " 15  BldgType       1460 non-null   object \n",
            " 16  HouseStyle     1460 non-null   object \n",
            " 17  OverallQual    1460 non-null   int64  \n",
            " 18  OverallCond    1460 non-null   int64  \n",
            " 19  YearBuilt      1460 non-null   int64  \n",
            " 20  YearRemodAdd   1460 non-null   int64  \n",
            " 21  RoofStyle      1460 non-null   object \n",
            " 22  RoofMatl       1460 non-null   object \n",
            " 23  Exterior1st    1460 non-null   object \n",
            " 24  Exterior2nd    1460 non-null   object \n",
            " 25  MasVnrType     588 non-null    object \n",
            " 26  MasVnrArea     1452 non-null   float64\n",
            " 27  ExterQual      1460 non-null   object \n",
            " 28  ExterCond      1460 non-null   object \n",
            " 29  Foundation     1460 non-null   object \n",
            " 30  BsmtQual       1423 non-null   object \n",
            " 31  BsmtCond       1423 non-null   object \n",
            " 32  BsmtExposure   1422 non-null   object \n",
            " 33  BsmtFinType1   1423 non-null   object \n",
            " 34  BsmtFinSF1     1460 non-null   int64  \n",
            " 35  BsmtFinType2   1422 non-null   object \n",
            " 36  BsmtFinSF2     1460 non-null   int64  \n",
            " 37  BsmtUnfSF      1460 non-null   int64  \n",
            " 38  TotalBsmtSF    1460 non-null   int64  \n",
            " 39  Heating        1460 non-null   object \n",
            " 40  HeatingQC      1460 non-null   object \n",
            " 41  CentralAir     1460 non-null   object \n",
            " 42  Electrical     1459 non-null   object \n",
            " 43  1stFlrSF       1460 non-null   int64  \n",
            " 44  2ndFlrSF       1460 non-null   int64  \n",
            " 45  LowQualFinSF   1460 non-null   int64  \n",
            " 46  GrLivArea      1460 non-null   int64  \n",
            " 47  BsmtFullBath   1460 non-null   int64  \n",
            " 48  BsmtHalfBath   1460 non-null   int64  \n",
            " 49  FullBath       1460 non-null   int64  \n",
            " 50  HalfBath       1460 non-null   int64  \n",
            " 51  BedroomAbvGr   1460 non-null   int64  \n",
            " 52  KitchenAbvGr   1460 non-null   int64  \n",
            " 53  KitchenQual    1460 non-null   object \n",
            " 54  TotRmsAbvGrd   1460 non-null   int64  \n",
            " 55  Functional     1460 non-null   object \n",
            " 56  Fireplaces     1460 non-null   int64  \n",
            " 57  FireplaceQu    770 non-null    object \n",
            " 58  GarageType     1379 non-null   object \n",
            " 59  GarageYrBlt    1379 non-null   float64\n",
            " 60  GarageFinish   1379 non-null   object \n",
            " 61  GarageCars     1460 non-null   int64  \n",
            " 62  GarageArea     1460 non-null   int64  \n",
            " 63  GarageQual     1379 non-null   object \n",
            " 64  GarageCond     1379 non-null   object \n",
            " 65  PavedDrive     1460 non-null   object \n",
            " 66  WoodDeckSF     1460 non-null   int64  \n",
            " 67  OpenPorchSF    1460 non-null   int64  \n",
            " 68  EnclosedPorch  1460 non-null   int64  \n",
            " 69  3SsnPorch      1460 non-null   int64  \n",
            " 70  ScreenPorch    1460 non-null   int64  \n",
            " 71  PoolArea       1460 non-null   int64  \n",
            " 72  PoolQC         7 non-null      object \n",
            " 73  Fence          281 non-null    object \n",
            " 74  MiscFeature    54 non-null     object \n",
            " 75  MiscVal        1460 non-null   int64  \n",
            " 76  MoSold         1460 non-null   int64  \n",
            " 77  YrSold         1460 non-null   int64  \n",
            " 78  SaleType       1460 non-null   object \n",
            " 79  SaleCondition  1460 non-null   object \n",
            " 80  SalePrice      1460 non-null   int64  \n",
            "dtypes: float64(3), int64(35), object(43)\n",
            "memory usage: 924.0+ KB\n",
            "None\n",
            "   Id  MSSubClass MSZoning  LotFrontage  LotArea Street Alley LotShape  \\\n",
            "0   1          60       RL         65.0     8450   Pave   NaN      Reg   \n",
            "1   2          20       RL         80.0     9600   Pave   NaN      Reg   \n",
            "2   3          60       RL         68.0    11250   Pave   NaN      IR1   \n",
            "3   4          70       RL         60.0     9550   Pave   NaN      IR1   \n",
            "4   5          60       RL         84.0    14260   Pave   NaN      IR1   \n",
            "\n",
            "  LandContour Utilities  ... PoolArea PoolQC Fence MiscFeature MiscVal MoSold  \\\n",
            "0         Lvl    AllPub  ...        0    NaN   NaN         NaN       0      2   \n",
            "1         Lvl    AllPub  ...        0    NaN   NaN         NaN       0      5   \n",
            "2         Lvl    AllPub  ...        0    NaN   NaN         NaN       0      9   \n",
            "3         Lvl    AllPub  ...        0    NaN   NaN         NaN       0      2   \n",
            "4         Lvl    AllPub  ...        0    NaN   NaN         NaN       0     12   \n",
            "\n",
            "  YrSold  SaleType  SaleCondition  SalePrice  \n",
            "0   2008        WD         Normal     208500  \n",
            "1   2007        WD         Normal     181500  \n",
            "2   2008        WD         Normal     223500  \n",
            "3   2006        WD        Abnorml     140000  \n",
            "4   2008        WD         Normal     250000  \n",
            "\n",
            "[5 rows x 81 columns]\n",
            "0\n",
            "SalePrice       1.000000\n",
            "OverallQual     0.784294\n",
            "GrLivArea       0.661325\n",
            "GarageCars      0.628013\n",
            "GarageArea      0.607230\n",
            "FullBath        0.577369\n",
            "YearBuilt       0.564558\n",
            "TotalBsmtSF     0.543508\n",
            "YearRemodAdd    0.541161\n",
            "1stFlrSF        0.522785\n",
            "Name: SalePrice, dtype: float64\n",
            "###############################################\n",
            "Root Mean Squared Error: 27535.374191615017\n",
            "R^2 Score: 0.8118445758514046\n",
            "###############################################\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd  # Import pandas library for data manipulation and analysis\n",
        "import numpy as np  # Import numpy for numerical operations\n",
        "from sklearn.model_selection import train_test_split  #function to split data\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error, r2_score #for metric evaluation\n",
        "\n",
        "# Load the training and testing data from CSV files\n",
        "train_df = pd.read_csv('train.csv')\n",
        "test_df = pd.read_csv('test.csv')\n",
        "\n",
        "# Print the structure of the training dataset and the first few rows to understand the data format\n",
        "print(train_df.info())\n",
        "print(train_df.head())\n",
        "\n",
        "# Drop columns with a high percentage of missing values, as they may not provide reliable information\n",
        "drop_cols = ['Alley', 'PoolQC', 'Fence', 'MiscFeature']\n",
        "train_df.drop(columns=drop_cols, inplace=True)\n",
        "\n",
        "# Impute missing values: fill missing numerical data with the median, and categorical data with the mode\n",
        "for col in train_df.columns:\n",
        "   if train_df[col].dtype == 'object':  # Check if the column data type is object (categorical)\n",
        "       train_df[col].fillna(train_df[col].mode()[0], inplace=True)  # Impute with mode for categorical columns\n",
        "   else:\n",
        "       train_df[col].fillna(train_df[col].median(), inplace=True)  # Impute with median for numerical columns\n",
        "\n",
        "# Confirm that all missing values have been filled by checking the maximum count of missing values across columns\n",
        "print(train_df.isnull().sum().max())  # Outputs 0 if no null values remain\n",
        "\n",
        "# Remove outliers from the `SalePrice` data using the Interquartile Range (IQR) method\n",
        "Q1 = train_df['SalePrice'].quantile(0.25)  # Calculate the first quartile (25th percentile)\n",
        "Q3 = train_df['SalePrice'].quantile(0.75)  # Calculate the third quartile (75th percentile)\n",
        "IQR = Q3 - Q1  # Calculate the interquartile range (distance between 25th and 75th percentiles)\n",
        "lower_bound = Q1 - 1.5 * IQR  # Define the lower bound for acceptable data points\n",
        "upper_bound = Q3 + 1.5 * IQR  # Define the upper bound for acceptable data points\n",
        "train_df = train_df[(train_df['SalePrice'] >= lower_bound) & (train_df['SalePrice'] <= upper_bound)]  # Filter out outliers\n",
        "\n",
        "# Select only numeric columns for correlation calculation\n",
        "numeric_features = train_df.select_dtypes(include=[np.number])\n",
        "corr = numeric_features.corr()['SalePrice'].sort_values(ascending=False)  # Calculate and sort correlation with SalePrice\n",
        "print(corr.head(10))  # Print the top 10 features most correlated with SalePrice\n",
        "\n",
        "# Define a list of features to be used for the model, based on correlation and domain knowledge\n",
        "selected_features = ['OverallQual', 'GrLivArea', 'GarageCars', 'TotalBsmtSF', 'FullBath', 'YearBuilt']\n",
        "\n",
        "# Prepare feature matrix (X) and target vector (y)\n",
        "X = train_df[selected_features]  # Features matrix\n",
        "y = train_df['SalePrice']  # Target vector\n",
        "\n",
        "# Split the dataset into training and testing sets with 80% training and 20% testing data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=45)\n",
        "\n",
        "# Create and train the Linear Regression model\n",
        "model = LinearRegression()\n",
        "model.fit(X_train, y_train)  # Fit the model on the training data\n",
        "\n",
        "# Predict the target variable for the test set\n",
        "y_pred = model.predict(X_test)  # Model predictions for the test set\n",
        "\n",
        "# Evaluate the model performance using Mean Squared Error (MSE) and R-squared metrics\n",
        "mse = mean_squared_error(y_test, y_pred)  # Calculate MSE\n",
        "rmse = np.sqrt(mse)  # Calculate Root Mean Squared Error (RMSE)\n",
        "r2 = r2_score(y_test, y_pred)  # Calculate R-squared value\n",
        "\n",
        "print(\"###############################################\")\n",
        "# Print evaluation metrics to assess model performance\n",
        "print(f\"Root Mean Squared Error: {rmse}\")\n",
        "print(f\"R^2 Score: {r2}\")\n",
        "print(\"###############################################\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ml_pyoRqgB0J"
      },
      "outputs": [],
      "source": [
        "\"\"\" The following characteristics were picked for the regression model's selection in order to forecast house prices:\n",
        "\n",
        "1. OverallQual (Overall Material and Finish Quality): As better construction and finishes are reflected in greater quality, higher quality is generally associated with higher pricing.\n",
        "\n",
        "2. GrLivArea (Above Grade Living Area): Buyers place a higher value on larger living areas, which has a direct impact on the market price of the property.\n",
        "\n",
        "3. GarageCars (Garage Size in Car Capacity): A larger garage improves the use and aesthetics of a house, which in turn affects the price and desirability of the property.\n",
        "\n",
        "4. TotalBsmtSF (Total Basement Area): A home's worth might rise dramatically if it has a larger basement because it usually translates into more usable space.\n",
        "\n",
        "5. FullBath (Full Bathrooms): Having more bathrooms increases luxury and convenience, which raises the market value and usefulness of the house.\n",
        "\n",
        "6. Year Built (Construction Year): While older homes may draw purchasers seeking for traditional elements and maybe lower prices, newer homes often command greater prices due to their modern conveniences and lesser upkeep requirements.\n",
        "\n",
        "Because these characteristics reflect significant elements that prospective purchasers usually take into consideration when determining a property's value, they are excellent indicators of home prices.\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_BbswDvnEX-k"
      },
      "source": [
        "# **Question 4: Using Pre-trained LLMs**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xKwKTnW1EX-k"
      },
      "source": [
        "(20 points)\n",
        "Utilize a **Pre-trained Language Model (PLM) from the Hugging Face Repository** for predicting sentiment polarities on the data you collected in Assignment 3.\n",
        "\n",
        "Then, choose a relevant LLM from their repository, such as GPT-3, BERT, or RoBERTa or any other related models.\n",
        "1. (5 points) Provide a brief description of the PLM you selected, including its original pretraining data sources,  number of parameters, and any task-specific fine-tuning if applied.\n",
        "2. (10 points) Use the selected PLM to perform the sentiment analysis on the data collected in Assignment 3. Only use the model in the **zero-shot** setting, NO finetuning is required. Evaluate performance of the model by comparing with the groundtruths (labels you annotated) on Accuracy, Precision, Recall, and F1 metrics.\n",
        "3. (5 points) Discuss the advantages and disadvantages of the selected PLM, and any challenges encountered during the implementation. This will enable a comprehensive understanding of the chosen LLM's applicability and effectiveness for the given task.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mci_VCN9lsqH",
        "outputId": "159842cf-4d0b-4875-816f-25c8c9fffa4e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                          clean_text predicted_sentiment\n",
            "0  shawshank redemption written directed frank da...            positive\n",
            "1  wonder film high rating quite literally breath...            positive\n",
            "2  im trying save money last film title consider ...            positive\n",
            "3  movie ordinary hollywood flick great deep mess...            positive\n",
            "4  oscar year shawshank redemption written direct...            negative\n",
            "5  best movie history best ending entertainment b...            positive\n",
            "6  one finest film made recent year poignant stor...            positive\n",
            "7  ive lost count number time seen movie one best...            positive\n",
            "8  misery stand best adaptation one add shawshank...            positive\n",
            "9  shawshank redemption without doubt one best fi...            positive\n",
            "Output saved to sentiment_predictions.csv\n",
            "\n",
            "Metrics:\n",
            "Accuracy: 0.90\n",
            "Precision: 1.00\n",
            "Recall: 0.90\n",
            "F1 Score: 0.95\n"
          ]
        }
      ],
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "from transformers import pipeline\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "# Load the dataset and preprocess it\n",
        "data_df = pd.read_csv('classified_sentiment_output.csv')\n",
        "data_df['Sentiment'] = data_df['Sentiment'].str.lower()  # Converting sentiment labels in the dataset to lowercase to ensure consistency\n",
        "data_subset = data_df.head(10).copy()  # Copying the first 10 entries from the dataset for analysis\n",
        "\n",
        "# Initializing the zero-shot classification model\n",
        "sentiment_classifier = pipeline(\"zero-shot-classification\", model=\"facebook/bart-large-mnli\")  # Loading the BART model pre-trained on MNLI\n",
        "\n",
        "# Defining labels and perform sentiment analysis\n",
        "labels = [\"positive\", \"negative\", \"neutral\"]  # Setting the target labels for sentiment classification\n",
        "# Applying the classifier to each text in the 'clean_text' column, store the highest confidence label in lowercase\n",
        "data_subset['predicted_sentiment'] = data_subset['clean_text'].apply(\n",
        "    lambda x: sentiment_classifier(x, candidate_labels=labels)['labels'][0].lower()\n",
        ")\n",
        "\n",
        "# results and displaying the output\n",
        "data_subset.to_csv('sentiment_predictions.csv', index=False)  # Saving the dataframe with predictions to a CSV file\n",
        "print(data_subset[['clean_text', 'predicted_sentiment']])  # Printing the cleaned text and its predicted sentiment\n",
        "print(\"Output saved to sentiment_predictions.csv\")\n",
        "\n",
        "# Calculating and displaying evaluation metrics\n",
        "# Creating a dictionary to store the metrics calculated from the actual and predicted labels\n",
        "metrics = {\n",
        "    'Accuracy': accuracy_score(data_subset['Sentiment'], data_subset['predicted_sentiment']),\n",
        "    'Precision': precision_score(data_subset['Sentiment'], data_subset['predicted_sentiment'], average='weighted', zero_division=1),\n",
        "    'Recall': recall_score(data_subset['Sentiment'], data_subset['predicted_sentiment'], average='weighted', zero_division=1),\n",
        "    'F1 Score': f1_score(data_subset['Sentiment'], data_subset['predicted_sentiment'], average='weighted', zero_division=1)\n",
        "}\n",
        "# Printing each metric\n",
        "print(\"\\nMetrics:\")\n",
        "for metric_name, metric_value in metrics.items():\n",
        "    print(f\"{metric_name}: {metric_value:.2f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\" 1. Overview of the Chosen Pre-trained Language Model (PLM)\n",
        "\n",
        "Model used: facebook/bart-large-mnli\n",
        "\n",
        "Overview:\n",
        "BART (Bidirectional and Auto-Regressive Transformers): Developed by Facebook AI, this model is tailored for both encoding and decoding tasks, making it flexible for a wide range of NLP applications such as text comprehension and generation.\n",
        "\n",
        "Sources for Pretraining:\n",
        "- BART was extensively trained on diverse textual sources, including books and a vast array of internet content, enabling it to grasp a wide spectrum of language variations and contexts.\n",
        "\n",
        "Model Size:\n",
        "- The facebook/bart-large-mnli variant boasts around 406 million parameters, which equips it with the ability to model intricate language patterns but also makes it computationally demanding.\n",
        "\n",
        "Specialized Fine-tuning:\n",
        "- This model has undergone fine-tuning with the Multi-Genre Natural Language Inference (MNLI) dataset, which is focused on text entailment tasks. This training enhances its capability to understand and interpret complex text relationships.\n",
        "\n",
        "2. Pros, Cons, and Challenges faced\n",
        "\n",
        "Pros:\n",
        "Flexibility:BART can handle various tasks beyond classification, including text summarization and language translation.\n",
        "Advanced Understanding: Its training on the MNLI dataset means BART is adept at analyzing detailed textual relationships, which is useful for deep linguistic tasks.\n",
        "Effective Performance: BART typically shows robust accuracy in relevant linguistic tasks due to its sophisticated understanding of language.\n",
        "\n",
        "Cons:\n",
        "High Resource Needs: The model’s vast number of parameters necessitates significant computational resources for operation, limiting its accessibility for users with constrained computational power.\n",
        "Risk of Overfitting: The complexity of BART, while beneficial for capturing nuanced patterns, also makes it susceptible to overfitting, particularly on smaller or homogeneous datasets.\n",
        "Inherent Biases: As with most large-scale models, BART may replicate biases inherent in its training data, potentially leading to skewed outputs.\n",
        "\n",
        "challenges faced:\n",
        "Resource Constraints: Executing this model for inference is resource-intensive, potentially challenging when handling extensive data sets.\n",
        "Setup and Compatibility: Integrating the Hugging Face pipeline with specific applications may require additional adjustments to ensure compatibility and functionality.\n",
        "Output Interpretation: Making sense of the model’s classifications and effectively using this information can be complex due to the abstract nature of its outputs.\n",
        "\n"
      ],
      "metadata": {
        "id": "si-sJK-KTkE2"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}