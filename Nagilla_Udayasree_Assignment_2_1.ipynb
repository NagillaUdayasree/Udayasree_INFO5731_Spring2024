{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NagillaUdayasree/Udayasree_INFO5731_Spring2024/blob/main/Nagilla_Udayasree_Assignment_2_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ryk8D1Q4Wsrp"
      },
      "source": [
        "# **INFO5731 Assignment 2**\n",
        "\n",
        "In this assignment, you will work on gathering text data from an open data source via web scraping or API. Following this, you will need to clean the text data and perform syntactic analysis on the data. Follow the instructions carefully and design well-structured Python programs to address each question.\n",
        "\n",
        "**Expectations**:\n",
        "*   Use the provided .*ipynb* document to write your code & respond to the questions. Avoid generating a new file.\n",
        "*   Write complete answers and run all the cells before submission.\n",
        "*   Make sure the submission is \"clean\"; *i.e.*, no unnecessary code cells.\n",
        "*   Once finished, allow shared rights from top right corner (*see Canvas for details*).\n",
        "\n",
        "* **Make sure to submit the cleaned data CSV in the comment section - 10 points**\n",
        "\n",
        "**Total points**: 100\n",
        "\n",
        "**Deadline**: Wednesday, at 11:59 PM.\n",
        "\n",
        "**Late Submission will have a penalty of 10% reduction for each day after the deadline.**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JkzR8cFAyGik"
      },
      "source": [
        "# Question 1 (40 points)\n",
        "\n",
        "Write a python program to collect text data from **either of the following sources** and save the data into a **csv file:**\n",
        "\n",
        "(1) Collect all the customer reviews of a product (you can choose any porduct) on amazon. [atleast 1000 reviews]\n",
        "\n",
        "(2) Collect the top 1000 User Reviews of a movie recently in 2023 or 2024 (you can choose any movie) from IMDB. [If one movie doesn't have sufficient reviews, collect reviews of atleast 2 or 3 movies]\n",
        "\n",
        "(3) Collect all the reviews of the top 1000 most popular software from G2 or Capterra.\n",
        "\n",
        "(4) Collect the **abstracts** of the top 10000 research papers by using the query \"machine learning\", \"data science\", \"artifical intelligence\", or \"information extraction\" from Semantic Scholar.\n",
        "\n",
        "(5) Collect all the information of the 904 narrators in the Densho Digital Repository.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "jDyTKYs-yGit",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e09510b7-6e88-46e2-cace-14140a7963a9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of user reviews: 1022\n",
            "Data saved to IMDB reviews.csv\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import csv\n",
        "\n",
        "#function to scrape user reviews from IMDb\n",
        "def get_user_reviews(url):\n",
        "    # Sending a GET request to the URL and parse the HTML content\n",
        "    page = requests.get(url)\n",
        "    soup = BeautifulSoup(page.content, 'html.parser')\n",
        "\n",
        "    # Finding all user review elements and extracting the review text\n",
        "    user_reviews_list = soup.find_all('div', 'text show-more__control')\n",
        "    user_reviews = [user_review.text.strip() for user_review in user_reviews_list]\n",
        "\n",
        "    # Extracting the token for pagination (used for accessing the loadmore feature to access next page)\n",
        "    ajax_token = None\n",
        "    ajax_token_list = soup.find_all('div', 'load-more-data')\n",
        "    if ajax_token_list:\n",
        "        ajax_token = ajax_token_list[-1].get('data-key')\n",
        "\n",
        "    return user_reviews, ajax_token\n",
        "\n",
        "# IMDb movie ID for 'The Shawshank Redemption'\n",
        "movie_imdb_id = 'tt0111161'\n",
        "url = f'https://www.imdb.com/title/{movie_imdb_id}/reviews/'\n",
        "\n",
        "# Scraping user reviews from the initial URL\n",
        "all_user_reviews, ajax_token = get_user_reviews(url)\n",
        "\n",
        "# Scraping additional user reviews using AJAX pagination\n",
        "for i in range(40):  # Loop to fetch 40 pages of reviews\n",
        "    ajax_url = f'{url}_ajax?ref_=undefined&paginationKey={ajax_token}'\n",
        "    user_reviews, ajax_token = get_user_reviews(ajax_url)\n",
        "    all_user_reviews += user_reviews\n",
        "\n",
        "# Printing the total number of user reviews collected\n",
        "print(f'Total number of user reviews: {len(all_user_reviews)}')\n",
        "\n",
        "#file path for saving the CSV file\n",
        "file_path = 'IMDB reviews.csv'\n",
        "\n",
        "# Open the CSV file in write mode and create a CSV writer object\n",
        "with open(file_path, 'w', newline='', encoding='utf-8') as file:\n",
        "    writer = csv.writer(file)\n",
        "\n",
        "    # Writing each user review to the CSV file\n",
        "    for review in all_user_reviews:\n",
        "        writer.writerow([review])\n",
        "\n",
        "# Printing a message indicating the successful saving of the CSV file\n",
        "print(f'Data saved to {file_path}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "90_NR8c5XGWc"
      },
      "source": [
        "# Question 2 (30 points)\n",
        "\n",
        "Write a python program to **clean the text data** you collected in the previous question and save the clean data in a new column in the csv file. The data cleaning steps include: [Code and output is required for each part]\n",
        "\n",
        "(1) Remove noise, such as special characters and punctuations.\n",
        "\n",
        "(2) Remove numbers.\n",
        "\n",
        "(3) Remove stopwords by using the stopwords list.\n",
        "\n",
        "(4) Lowercase all texts\n",
        "\n",
        "(5) Stemming.\n",
        "\n",
        "(6) Lemmatization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "id": "5QX6bJjGWXY9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "39e47685-2d60-4659-a5c9-e9a6235ae573"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Noiseless Data saved to Noiseless_IMDB_reviews.csv\n",
            "Non-numeric Data saved to Non_numeric_IMDB_reviews.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data after removing stopwrods is saved to No_Stopwords_IMDB_reviews.csv\n",
            "Data after converting to lower case is saved to Lower_case_IMDB_reviews.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data after stemming is saved to stemmed_reviews.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data after Lemmatizing is saved to lemmatized_reviews.csv\n",
            "###################################################\n",
            "Cleaned data is saved into Cleaned_data.csv file\n",
            "###################################################\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "import pandas as pd\n",
        "import csv\n",
        "import nltk\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.corpus import stopwords\n",
        "from textblob import Word\n",
        "\n",
        "\n",
        "# Reading the CSV file into a DataFrame\n",
        "df = pd.read_csv('IMDB reviews.csv', header=None)\n",
        "\n",
        "#Removing Noise\n",
        "# Defining the regular expression pattern to remove noise (special characters and punctuations)\n",
        "pattern = r'[^\\w\\s]'\n",
        "# regex substitution to remove special characters from the text column\n",
        "df[0] = df[0].apply(lambda x: re.sub(pattern, '', x))\n",
        "#Writing the DataFrame to a new file\n",
        "df.to_csv('Noiseless_IMDB_reviews.csv', index=False, header=False)\n",
        "print(f'Noiseless Data saved to Noiseless_IMDB_reviews.csv')\n",
        "\n",
        "#writing the new dataframe to perform operations on the previous output file\n",
        "df_1=pd.read_csv('Noiseless_IMDB_reviews.csv', header=None)\n",
        "\n",
        "#Removing Numbers\n",
        "# Defining the regular expression pattern to remove numbers\n",
        "pattern = r'\\d'\n",
        "# regex substitution to remove numbers from the text column\n",
        "df_1[0] = df_1[0].apply(lambda x: re.sub(pattern, '', x))\n",
        "# Writing the DataFrame to a new file\n",
        "df_1.to_csv('Non_numeric_IMDB_reviews.csv', index=False, header=False)\n",
        "print(f'Non-numeric Data saved to Non_numeric_IMDB_reviews.csv')\n",
        "\n",
        "#writing the new dataframe to perform operations on the previous output file\n",
        "df_2=pd.read_csv('Non_numeric_IMDB_reviews.csv', header=None)\n",
        "\n",
        "#Removing stopwords\n",
        "nltk.download('stopwords')\n",
        "# Import English stopwords from NLTK\n",
        "stop_words = stopwords.words('english')\n",
        "# Removing stop words from the text column\n",
        "df_2[0] = df_2[0].apply(lambda x: ' '.join([word for word in x.split() if word.lower() not in stop_words]))\n",
        "# Writing the DataFrame to a new file\n",
        "df_2.to_csv('No_Stopwords_IMDB_reviews.csv', index=False, header=False)\n",
        "print(f'Data after removing stopwrods is saved to No_Stopwords_IMDB_reviews.csv')\n",
        "\n",
        "#writing the new dataframe to perform operations on the previous output file\n",
        "df_3=pd.read_csv('No_Stopwords_IMDB_reviews.csv', header=None)\n",
        "\n",
        "#converting the text to Lower case\n",
        "df_3[0] = df_3[0].apply(lambda x: \" \".join(x.lower() for x in x.split()))\n",
        "df_3[0].head()\n",
        "# Writing the DataFrame to a new file\n",
        "df_3.to_csv('Lower_case_IMDB_reviews.csv',index=False, header=False)\n",
        "print(f'Data after converting to lower case is saved to Lower_case_IMDB_reviews.csv')\n",
        "\n",
        "#writing the new dataframe to perform operations on the previous output file\n",
        "df_4=pd.read_csv('Lower_case_IMDB_reviews.csv', header=None)\n",
        "\n",
        "#Stemming\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('punkt')\n",
        "# Initialize Porter Stemmer\n",
        "stemmer = PorterStemmer()\n",
        "#function to perform stemming on a text\n",
        "def stem_text(text):\n",
        "    # Tokenize the text into words\n",
        "    words = word_tokenize(text)\n",
        "    # Applying stemming to each word and join them back into a sentence\n",
        "    stemmed_words = [stemmer.stem(word) for word in words]\n",
        "    stemmed_text = ' '.join(stemmed_words)\n",
        "    return stemmed_text\n",
        "# Applying stemming to the column\n",
        "df_4[0] = df_4[0].apply(stem_text)\n",
        "# Saving the DataFrame with stemmed text to a new CSV file\n",
        "df_4.to_csv('stemmed_reviews.csv', index=False)\n",
        "print(f'Data after stemming is saved to stemmed_reviews.csv')\n",
        "\n",
        "#writing the new dataframe to perform operations on the previous output file\n",
        "df_5=pd.read_csv('Lower_case_IMDB_reviews.csv', header=None)\n",
        "\n",
        "#Lemmatization\n",
        "\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# Initializing the WordNet Lemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# Function to lemmatize a sentence\n",
        "def lemmatize_text(text):\n",
        "    # Tokenize the sentence into words\n",
        "    words = word_tokenize(text)\n",
        "    # Lemmatizing each word and join them back into a sentence\n",
        "    lemmatized_text = ' '.join([lemmatizer.lemmatize(word) for word in words])\n",
        "    return lemmatized_text\n",
        "\n",
        "# Applying lemmatization to the coulmn\n",
        "df_5[0] = df_5[0].apply(lemmatize_text)\n",
        "# Save the DataFrame with lemmatized text to a new CSV file\n",
        "df_5.to_csv('lemmatized_reviews.csv', index=False)\n",
        "print(f'Data after Lemmatizing is saved to lemmatized_reviews.csv')\n",
        "print(\"###################################################\")\n",
        "print(\"Cleaned data is saved into Cleaned_data.csv file\")\n",
        "print(\"###################################################\")\n",
        "df_5.to_csv('Cleaned_data.csv')\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1F_PZdH9Sh49"
      },
      "source": [
        "# Question 3 (30 points)\n",
        "\n",
        "Write a python program to **conduct syntax and structure analysis of the clean text** you just saved above. The syntax and structure analysis includes:\n",
        "\n",
        "(1) **Parts of Speech (POS) Tagging:** Tag Parts of Speech of each word in the text, and calculate the total number of N(oun), V(erb), Adj(ective), Adv(erb), respectively.\n",
        "\n",
        "(2) **Constituency Parsing and Dependency Parsing:** print out the constituency parsing trees and dependency parsing trees of all the sentences. Using one sentence as an example to explain your understanding about the constituency parsing tree and dependency parsing tree.\n",
        "\n",
        "(3) **Named Entity Recognition:** Extract all the entities such as person names, organizations, locations, product names, and date from the clean texts, calculate the count of each entity."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "id": "Y0oOSlsOS0cq",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "4e963c93-fb99-4cb5-e1a9-c4f9c53fc0ac"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: benepar in /usr/local/lib/python3.10/dist-packages (0.2.0)\n",
            "Requirement already satisfied: nltk>=3.2 in /usr/local/lib/python3.10/dist-packages (from benepar) (3.8.1)\n",
            "Requirement already satisfied: spacy>=2.0.9 in /usr/local/lib/python3.10/dist-packages (from benepar) (3.7.4)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from benepar) (2.1.0+cu121)\n",
            "Requirement already satisfied: torch-struct>=0.5 in /usr/local/lib/python3.10/dist-packages (from benepar) (0.5)\n",
            "Requirement already satisfied: tokenizers>=0.9.4 in /usr/local/lib/python3.10/dist-packages (from benepar) (0.15.2)\n",
            "Requirement already satisfied: transformers[tokenizers,torch]>=4.2.2 in /usr/local/lib/python3.10/dist-packages (from benepar) (4.37.2)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from benepar) (3.20.3)\n",
            "Requirement already satisfied: sentencepiece>=0.1.91 in /usr/local/lib/python3.10/dist-packages (from benepar) (0.1.99)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk>=3.2->benepar) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk>=3.2->benepar) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk>=3.2->benepar) (2023.12.25)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk>=3.2->benepar) (4.66.2)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy>=2.0.9->benepar) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy>=2.0.9->benepar) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy>=2.0.9->benepar) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy>=2.0.9->benepar) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy>=2.0.9->benepar) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy>=2.0.9->benepar) (8.2.3)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy>=2.0.9->benepar) (1.1.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy>=2.0.9->benepar) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy>=2.0.9->benepar) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy>=2.0.9->benepar) (0.3.4)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy>=2.0.9->benepar) (0.9.0)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy>=2.0.9->benepar) (6.4.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy>=2.0.9->benepar) (2.31.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy>=2.0.9->benepar) (2.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy>=2.0.9->benepar) (3.1.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy>=2.0.9->benepar) (67.7.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy>=2.0.9->benepar) (23.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy>=2.0.9->benepar) (3.3.0)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy>=2.0.9->benepar) (1.25.2)\n",
            "Requirement already satisfied: huggingface_hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from tokenizers>=0.9.4->benepar) (0.20.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->benepar) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->benepar) (4.9.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->benepar) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->benepar) (3.2.1)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->benepar) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->benepar) (2.1.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers[tokenizers,torch]>=4.2.2->benepar) (6.0.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers[tokenizers,torch]>=4.2.2->benepar) (0.4.2)\n",
            "Requirement already satisfied: accelerate>=0.21.0 in /usr/local/lib/python3.10/dist-packages (from transformers[tokenizers,torch]>=4.2.2->benepar) (0.27.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.21.0->transformers[tokenizers,torch]>=4.2.2->benepar) (5.9.5)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy>=2.0.9->benepar) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.16.2 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy>=2.0.9->benepar) (2.16.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.0.9->benepar) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.0.9->benepar) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.0.9->benepar) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.0.9->benepar) (2024.2.2)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy>=2.0.9->benepar) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy>=2.0.9->benepar) (0.1.4)\n",
            "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.4.0,>=0.1.0->spacy>=2.0.9->benepar) (0.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy>=2.0.9->benepar) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.6.0->benepar) (1.3.0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package benepar_en3 to /root/nltk_data...\n",
            "[nltk_data]   Package benepar_en3 is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "##################################################################################\n",
            "POS Tagging Output\n",
            "Total Nouns: 42480\n",
            "Total Verbs : 18509\n",
            "Total Adjectives : 20973\n",
            "Total Adverbs: 8526\n",
            "##################################################################################\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                   TOP                               \n",
            "                    |                                 \n",
            "                    NP                               \n",
            "        ____________|_____                            \n",
            "       NP                 NP                         \n",
            "  _____|______       _____|_____________________      \n",
            "JJS    NN     NN   JJS   VBG         NN         NN   \n",
            " |     |      |     |     |          |          |     \n",
            "best movie history best ending entertainment business\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.SVG object>"
            ],
            "image/svg+xml": "<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xml:lang=\"en\" id=\"7967edd50e044475ac2a805f6550a624-0\" class=\"displacy\" width=\"750\" height=\"237.0\" direction=\"ltr\" style=\"max-width: none; height: 237.0px; color: #000000; background: #ffffff; font-family: Arial; direction: ltr\">\n<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"147.0\">\n    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">best</tspan>\n    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">ADJ</tspan>\n</text>\n\n<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"147.0\">\n    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"150\">movie</tspan>\n    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"150\">NOUN</tspan>\n</text>\n\n<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"147.0\">\n    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"250\">history</tspan>\n    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"250\">NOUN</tspan>\n</text>\n\n<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"147.0\">\n    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"350\">best</tspan>\n    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"350\">ADV</tspan>\n</text>\n\n<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"147.0\">\n    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"450\">ending</tspan>\n    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"450\">VERB</tspan>\n</text>\n\n<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"147.0\">\n    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"550\">entertainment</tspan>\n    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"550\">NOUN</tspan>\n</text>\n\n<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"147.0\">\n    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"650\">business</tspan>\n    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"650\">NOUN</tspan>\n</text>\n\n<g class=\"displacy-arrow\">\n    <path class=\"displacy-arc\" id=\"arrow-7967edd50e044475ac2a805f6550a624-0-0\" stroke-width=\"2px\" d=\"M62,102.0 62,68.66666666666666 250.0,68.66666666666666 250.0,102.0\" fill=\"none\" stroke=\"currentColor\"/>\n    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n        <textPath xlink:href=\"#arrow-7967edd50e044475ac2a805f6550a624-0-0\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">amod</textPath>\n    </text>\n    <path class=\"displacy-arrowhead\" d=\"M62,104.0 L58,96.0 66,96.0\" fill=\"currentColor\"/>\n</g>\n\n<g class=\"displacy-arrow\">\n    <path class=\"displacy-arc\" id=\"arrow-7967edd50e044475ac2a805f6550a624-0-1\" stroke-width=\"2px\" d=\"M162,102.0 162,85.33333333333333 247.0,85.33333333333333 247.0,102.0\" fill=\"none\" stroke=\"currentColor\"/>\n    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n        <textPath xlink:href=\"#arrow-7967edd50e044475ac2a805f6550a624-0-1\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">compound</textPath>\n    </text>\n    <path class=\"displacy-arrowhead\" d=\"M162,104.0 L158,96.0 166,96.0\" fill=\"currentColor\"/>\n</g>\n\n<g class=\"displacy-arrow\">\n    <path class=\"displacy-arc\" id=\"arrow-7967edd50e044475ac2a805f6550a624-0-2\" stroke-width=\"2px\" d=\"M362,102.0 362,85.33333333333333 447.0,85.33333333333333 447.0,102.0\" fill=\"none\" stroke=\"currentColor\"/>\n    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n        <textPath xlink:href=\"#arrow-7967edd50e044475ac2a805f6550a624-0-2\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">advmod</textPath>\n    </text>\n    <path class=\"displacy-arrowhead\" d=\"M362,104.0 L358,96.0 366,96.0\" fill=\"currentColor\"/>\n</g>\n\n<g class=\"displacy-arrow\">\n    <path class=\"displacy-arc\" id=\"arrow-7967edd50e044475ac2a805f6550a624-0-3\" stroke-width=\"2px\" d=\"M262,102.0 262,68.66666666666666 450.0,68.66666666666666 450.0,102.0\" fill=\"none\" stroke=\"currentColor\"/>\n    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n        <textPath xlink:href=\"#arrow-7967edd50e044475ac2a805f6550a624-0-3\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">acl</textPath>\n    </text>\n    <path class=\"displacy-arrowhead\" d=\"M450.0,104.0 L454.0,96.0 446.0,96.0\" fill=\"currentColor\"/>\n</g>\n\n<g class=\"displacy-arrow\">\n    <path class=\"displacy-arc\" id=\"arrow-7967edd50e044475ac2a805f6550a624-0-4\" stroke-width=\"2px\" d=\"M562,102.0 562,85.33333333333333 647.0,85.33333333333333 647.0,102.0\" fill=\"none\" stroke=\"currentColor\"/>\n    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n        <textPath xlink:href=\"#arrow-7967edd50e044475ac2a805f6550a624-0-4\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">compound</textPath>\n    </text>\n    <path class=\"displacy-arrowhead\" d=\"M562,104.0 L558,96.0 566,96.0\" fill=\"currentColor\"/>\n</g>\n\n<g class=\"displacy-arrow\">\n    <path class=\"displacy-arc\" id=\"arrow-7967edd50e044475ac2a805f6550a624-0-5\" stroke-width=\"2px\" d=\"M462,102.0 462,68.66666666666666 650.0,68.66666666666666 650.0,102.0\" fill=\"none\" stroke=\"currentColor\"/>\n    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n        <textPath xlink:href=\"#arrow-7967edd50e044475ac2a805f6550a624-0-5\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">dobj</textPath>\n    </text>\n    <path class=\"displacy-arrowhead\" d=\"M650.0,104.0 L654.0,96.0 646.0,96.0\" fill=\"currentColor\"/>\n</g>\n</svg>"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "##################################################################################\n",
            "NER Output\n",
            "Entities of type 'PERSON':\n",
            "tim: 2\n",
            "james whitmore: 2\n",
            "andy: 2\n",
            "stephen king: 2\n",
            "andy dufresne: 1\n",
            "freeman freeman: 1\n",
            "rita hayworth: 1\n",
            "dvd tim robbins: 1\n",
            "desson thomson: 1\n",
            "andy robbins: 1\n",
            "zihuatanejo: 1\n",
            "andy first: 1\n",
            "carefree andy: 1\n",
            "bob gunton: 1\n",
            "freeman: 1\n",
            "freeman tim robbins: 1\n",
            "\n",
            "Entities of type 'ORG':\n",
            "brooks halten: 1\n",
            "\n",
            "Entities of type 'GPE':\n",
            "washington: 1\n",
            "\n",
            "Entities of type 'PRODUCT':\n",
            "\n",
            "Entities of type 'DATE':\n",
            "years: 3\n",
            "oscar year: 1\n",
            "today: 1\n",
            "day day: 1\n",
            "satan: 1\n",
            "recent years: 1\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import nltk\n",
        "from collections import Counter\n",
        "!pip install benepar\n",
        "import benepar\n",
        "import spacy\n",
        "benepar.download('benepar_en3')\n",
        "\n",
        "\n",
        "\n",
        "# Downloading NLTK resources (necessary for POS tagging)\n",
        "nltk.download('punkt')  # Tokenizer\n",
        "nltk.download('averaged_perceptron_tagger')  # POS tagger\n",
        "\n",
        "# Loading the CSV file into a DataFrame\n",
        "df_6 = pd.read_csv('lemmatized_reviews.csv')\n",
        "\n",
        "# Function to tokenize and tag each word in the text\n",
        "def pos_tagging(text):\n",
        "    # Tokenize the text into words\n",
        "    tokens = nltk.word_tokenize(text)\n",
        "    # Tagging each word with its part of speech (POS)\n",
        "    pos_tags = nltk.pos_tag(tokens)\n",
        "    return pos_tags\n",
        "\n",
        "# Applying POS tagging to the column and create a new column 'pos_tags'\n",
        "df_6['0'] = df_6['0'].apply(pos_tagging)\n",
        "\n",
        "# Initializing counters for each POS tag\n",
        "noun_count = 0\n",
        "verb_count = 0\n",
        "adj_count = 0\n",
        "adv_count = 0\n",
        "\n",
        "# Function to count POS tags\n",
        "def count_pos_tags(pos_tags):\n",
        "    global noun_count, verb_count, adj_count, adv_count\n",
        "    # Iterating through each word-tag tuple\n",
        "    for _, tag in pos_tags:\n",
        "        if tag.startswith('N'):  # Noun\n",
        "            noun_count += 1\n",
        "        elif tag.startswith('V'):  # Verb\n",
        "            verb_count += 1\n",
        "        elif tag.startswith('J'):  # Adjective\n",
        "            adj_count += 1\n",
        "        elif tag.startswith('R'):  # Adverb\n",
        "            adv_count += 1\n",
        "\n",
        "# Applying the count_pos_tags function to each element in the 'pos_tags' column\n",
        "df_6['0'].apply(count_pos_tags)\n",
        "\n",
        "print(\"##################################################################################\")\n",
        "print(\"POS Tagging Output\")\n",
        "# Printing the total counts of each POS tag\n",
        "print(\"Total Nouns:\", noun_count)\n",
        "print(\"Total Verbs :\", verb_count)\n",
        "print(\"Total Adjectives :\", adj_count)\n",
        "print(\"Total Adverbs:\", adv_count)\n",
        "\n",
        "\n",
        "print(\"##################################################################################\")\n",
        "\n",
        "\n",
        "import spacy\n",
        "import csv\n",
        "from benepar import Parser\n",
        "from spacy import displacy\n",
        "from IPython.display import SVG\n",
        "import warnings\n",
        "\n",
        "# Suppress all warnings which are not impactful to maintain the output window clean\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "#function display the output in a graphical format\n",
        "def display_svg(svg_code):\n",
        "    display(SVG(svg_code))\n",
        "\n",
        "def get_parse_trees(text):\n",
        "    # Loading spaCy model for tokenization and sentence segmentation\n",
        "    nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "    # Loading Benepar model for constituency parsing\n",
        "    benepar_model = Parser(\"benepar_en3\")\n",
        "\n",
        "    doc = nlp(text)\n",
        "\n",
        "    constituency_tree = None\n",
        "    dependency_tree = None\n",
        "\n",
        "    # Constituency Parsing with Benepar\n",
        "    try:\n",
        "        constituency_tree = benepar_model.parse(text).pretty_print()\n",
        "    except Exception as e:  # Catch potential errors\n",
        "        constituency_tree = f\"Error parsing constituency: {e}\"\n",
        "\n",
        "    # Dependency Parsing (using spaCy)\n",
        "    dependency_tree_svg = displacy.render(doc, style=\"dep\", jupyter=False, options={\"compact\": True, \"distance\": 100})\n",
        "\n",
        "    return constituency_tree, dependency_tree_svg\n",
        "\n",
        "sentences = df_7[0]\n",
        "\n",
        "# Choosing one sentence for console output\n",
        "sample_sentence = sentences[6:7]\n",
        "\n",
        "parse_trees = []\n",
        "for sentence in sample_sentence:\n",
        "    constituency_tree, dependency_tree_svg = get_parse_trees(sentence)\n",
        "    parse_trees.append((sentence, constituency_tree, dependency_tree_svg))\n",
        "\n",
        "\n",
        "for sentence, constituency_tree, dependency_tree_svg in parse_trees:\n",
        "    display_svg(dependency_tree_svg)\n",
        "\n",
        "\n",
        "print(\"##################################################################################\")\n",
        "\n",
        "\n",
        "#NER operation\n",
        "# Reading the CSV file into new data frame\n",
        "df_9 = pd.read_csv('Lower_case_IMDB_reviews.csv')\n",
        "\n",
        "# Rename the first row to 0\n",
        "df_9.columns.values[0] = '0'\n",
        "\n",
        "#writing the modified DataFrame back to a CSV file\n",
        "df_9.to_csv('modified_Lower case_file.csv', index=False)\n",
        "\n",
        "# Loading spaCy model with NER component\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Function to perform NER on text data and calculate entity counts\n",
        "def extract_entities_and_counts(texts):\n",
        "    # Initializing counters for each entity type\n",
        "    entity_counts = {\n",
        "        \"PERSON\": Counter(),\n",
        "        \"ORG\": Counter(),\n",
        "        \"GPE\": Counter(),\n",
        "        \"PRODUCT\": Counter(),\n",
        "        \"DATE\": Counter()\n",
        "    }\n",
        "\n",
        "    # Processing each text\n",
        "    for text in texts:\n",
        "        doc = nlp(text)\n",
        "        # Extracting entities from the document\n",
        "        for ent in doc.ents:\n",
        "            # Checking if the entity is a person, organization, location, product, or date\n",
        "            if ent.label_ in entity_counts:\n",
        "                entity_counts[ent.label_][ent.text] += 1\n",
        "\n",
        "    return entity_counts\n",
        "\n",
        "# Reading the DataFrame from the CSV file\n",
        "df_9 = pd.read_csv('modified_Lower case_file.csv')\n",
        "\n",
        "# Access the clean text data from the appropriate column\n",
        "texts = df_9['0'].tolist()\n",
        "\n",
        "# Extracting entities and calculate counts for few sentences\n",
        "entity_counts = extract_entities_and_counts(texts[2:7])\n",
        "\n",
        "print(\"NER Output\")\n",
        "# Printing counts for each entity type\n",
        "for entity_type, counts in entity_counts.items():\n",
        "    print(f\"Entities of type '{entity_type}':\")\n",
        "    for entity, count in counts.most_common():\n",
        "        print(f\"{entity}: {count}\")\n",
        "    print()\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Mandatory Question\n",
        "\n",
        "Provide your thoughts on the assignment. What did you find challenging, and what aspects did you enjoy? Your opinion on the provided time to complete the assignment."
      ],
      "metadata": {
        "id": "q8BFCvWp32cf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Write your response below"
      ],
      "metadata": {
        "id": "_e557s2w4BpK"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}